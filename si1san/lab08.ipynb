{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yyyyy7105/Course-Projects/blob/main/si1san/lab08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0df50c3a",
      "metadata": {
        "id": "0df50c3a"
      },
      "source": [
        "# CSC413 Lab 8: Text Generation with Transformers\n",
        "\n",
        "In this lab, we will build a generative transformer to generate\n",
        "new lines that emulates the TV show [Friends](https://www.imdb.com/title/tt0108778/).\n",
        "In order to do so, we will leverage\n",
        "Andrej Karpathy's implementation of GPT2 called the\n",
        "[nanoGPT](https://github.com/karpathy/nanoGPT).\n",
        "This particular implementation uses a small number of components and\n",
        "focuses on the essential ideas behind the GPT2 model.\n",
        "\n",
        "Instead of training a GPT model from scratch, we will *fine-tune*\n",
        "a pre-trained model. This reduces training time necessary to achieve\n",
        "a reasonable measure of performance.\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "\n",
        "- Fine-tune a transformer model on a new data set.\n",
        "- Trace the execution of a transformer model to explain its inner working.\n",
        "- Compare the batching approach used here and in the pervious lab.\n",
        "- Explain the ethical issues involved in applying large language models.\n",
        "\n",
        "Acknowledgements:\n",
        "\n",
        "- The nanoGPT implementation is from https://github.com/karpathy/nanoGPT\n",
        "- Data is from https://convokit.cornell.edu/documentation/friends.html\n",
        "- The Byte Pair Encoding tokenizer is from https://github.com/openai/tiktoken\n",
        "- GPT2 Model was introduced in the paper [Language Models are Unsupervised Multitask Learners](https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf)\n",
        "\n",
        "Please work in groups of 1-2 during the lab.\n",
        "\n",
        "## Submission\n",
        "\n",
        "If you are working with a partner, start by creating a group on Markus.\n",
        "If you are working alone, click \"Working Alone\".\n",
        "\n",
        "Submit the ipynb file `lab08.pdf` on Markus\n",
        "**containing all your solutions to the Graded Task**s.\n",
        "Your notebook file must contain your code **and outputs** where applicable,\n",
        "including printed lines and images.\n",
        "Your TA will not run your code for the purpose of grading.\n",
        "\n",
        "For this lab, you should submit the following:\n",
        "\n",
        "- Part 1. Your code to generate the list `uts`. (1 point)\n",
        "- Part 1. Your explanation for why the tokenization method needs to be consistent. (1 point)\n",
        "- Part 1. Your explanation of why the target tensor is an \"offset\" of the input tensor. (1 point)\n",
        "- Part 2. Your implementation of the `generate` function (4 points) and 1 points for a sensible test of your design. (5 points)\n",
        "- Part 2. Your explanation of the relationship between `lm_head` and `wte`. (1 point)\n",
        "- Part 2. Your result for the shape of `x` in the `GPT.forward()` method. (1 point)\n",
        "- Part 2. Your implementation of the missing attention code. (4 points)\n",
        "- Part 2. Your computation of the shape of `(q @ k.transpose(-2, -1))` in the causal self attention module (1 point)\n",
        "- Part 2. Your explantion of why masking makes sense intuitively for causal self attention. (1 point)\n",
        "- Part 2. Your computation of the number of parameters in a GPT2 model. (3 points)\n",
        "\n",
        "## Part 1. Data\n",
        "\n",
        "The \"Friends Corpus\" can be downloaded through the ConvKit package on\n",
        "Python. You can read more about the data\n",
        "[here](https://convokit.cornell.edu/documentation/friends.html).\n",
        "\n",
        "Let's install this package, and explore the data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d07723d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d07723d0",
        "outputId": "ebfdac92-46ca-4efb-f2f4-8a93ad56786d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting convokit\n",
            "  Downloading convokit-3.0.0.tar.gz (183 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.2/183.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.7.1)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (2.0.3)\n",
            "Collecting msgpack-numpy>=0.4.3.2 (from convokit)\n",
            "  Downloading msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: spacy>=2.3.5 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.7.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.2.2)\n",
            "Requirement already satisfied: nltk>=3.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.8.1)\n",
            "Collecting dill>=0.2.9 (from convokit)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.3.2)\n",
            "Collecting clean-text>=0.6.0 (from convokit)\n",
            "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
            "Collecting unidecode>=1.1.1 (from convokit)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (4.66.2)\n",
            "Collecting pymongo>=4.0 (from convokit)\n",
            "  Downloading pymongo-4.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (676 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m676.9/676.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from convokit) (6.0.1)\n",
            "Collecting dnspython>=1.16.0 (from convokit)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting emoji<2.0.0,>=1.0.0 (from clean-text>=0.6.0->convokit)\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy<7.0,>=6.0 (from clean-text>=0.6.0->convokit)\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (2.8.2)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from msgpack-numpy>=0.4.3.2->convokit) (1.0.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (2023.12.25)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->convokit) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->convokit) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->convokit) (3.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (6.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.3.0)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0,>=6.0->clean-text>=0.6.0->convokit) (0.2.13)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (4.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->convokit) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.3.5->convokit) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.3.5->convokit) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy>=2.3.5->convokit) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=2.3.5->convokit) (2.1.5)\n",
            "Building wheels for collected packages: convokit, emoji\n",
            "  Building wheel for convokit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for convokit: filename=convokit-3.0.0-py3-none-any.whl size=216707 sha256=87ab5512d7b20693fcf5cf4a6d5414f13b95047fab8c02923cbb62d2f281705a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/89/8c/2677fdb888588b6f93cb6ac86bdfb020f1f1c33e0d5525b231\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171034 sha256=8db16b51e1ce99eb3bf5c488c280c959def5ccea804cc35c326c83a8e46c32c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n",
            "Successfully built convokit emoji\n",
            "Installing collected packages: emoji, unidecode, msgpack-numpy, ftfy, dnspython, dill, pymongo, clean-text, convokit\n",
            "Successfully installed clean-text-0.6.0 convokit-3.0.0 dill-0.3.8 dnspython-2.6.1 emoji-1.7.0 ftfy-6.2.0 msgpack-numpy-0.4.8 pymongo-4.6.3 unidecode-1.3.8\n"
          ]
        }
      ],
      "source": [
        "%pip install convokit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba56c84b",
      "metadata": {
        "id": "ba56c84b"
      },
      "outputs": [],
      "source": [
        "import convokit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebf68b49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebf68b49",
        "outputId": "38d90704-5ebd-4b34-8344-60d435c5aea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading friends-corpus to /root/.convokit/downloads/friends-corpus\n",
            "Downloading friends-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/friends-corpus/friends-corpus.zip (6.1MB)... Done\n",
            "No configuration file found at /root/.convokit/config.yml; writing with contents: \n",
            "# Default Backend Parameters\n",
            "db_host: localhost:27017\n",
            "data_directory: ~/.convokit/saved-corpora\n",
            "default_backend: mem\n"
          ]
        }
      ],
      "source": [
        "corpus = convokit.Corpus(convokit.download('friends-corpus'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0357230",
      "metadata": {
        "id": "d0357230"
      },
      "source": [
        "**Task**: Run the code below, which iterates through the first 10 utterances\n",
        "of the show. How is each utterance formatted? What do the `speaker`\n",
        "and `text` field mean?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f40ef3b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f40ef3b",
        "outputId": "74abc7bc-78f6-4b33-c17d-35dc97296f3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utterance(id: 's01_e01_c01_u001', conversation_id: s01_e01_c01_u001, reply-to: None, speaker: Speaker(id: 'Monica Geller', vectors: [], meta: ConvoKitMeta({})), timestamp: None, text: \"There's nothing to tell! He's just some guy I work with!\", vectors: [], meta: ConvoKitMeta({'tokens': [['There', \"'s\", 'nothing', 'to', 'tell', '!'], ['He', \"'s\", 'just', 'some', 'guy', 'I', 'work', 'with', '!']], 'character_entities': [[], [[0, 1, 'Paul the Wine Guy'], [4, 5, 'Paul the Wine Guy'], [5, 6, 'Monica Geller']]], 'emotion': None, 'caption': None, 'transcript_with_note': None, 'tokens_with_note': None}))\n",
            "Utterance(id: 's01_e01_c01_u002', conversation_id: s01_e01_c01_u001, reply-to: s01_e01_c01_u001, speaker: Speaker(id: 'Joey Tribbiani', vectors: [], meta: ConvoKitMeta({})), timestamp: None, text: \"C'mon, you're going out with the guy! There's gotta be something wrong with him!\", vectors: [], meta: ConvoKitMeta({'tokens': [[\"C'mon\", ',', 'you', \"'re\", 'going', 'out', 'with', 'the', 'guy', '!'], ['There', \"'s\", 'got', 'ta', 'be', 'something', 'wrong', 'with', 'him', '!']], 'character_entities': [[[2, 3, 'Monica Geller'], [8, 9, 'Paul the Wine Guy']], [[8, 9, 'Paul the Wine Guy']]], 'emotion': None, 'caption': None, 'transcript_with_note': None, 'tokens_with_note': None}))\n",
            "Utterance(id: 's01_e01_c01_u003', conversation_id: s01_e01_c01_u001, reply-to: s01_e01_c01_u002, speaker: Speaker(id: 'Chandler Bing', vectors: [], meta: ConvoKitMeta({})), timestamp: None, text: 'All right Joey, be nice. So does he have a hump? A hump and a hairpiece?', vectors: [], meta: ConvoKitMeta({'tokens': [['All', 'right', 'Joey', ',', 'be', 'nice', '.'], ['So', 'does', 'he', 'have', 'a', 'hump', '?'], ['A', 'hump', 'and', 'a', 'hairpiece', '?']], 'character_entities': [[[2, 3, 'Joey Tribbiani']], [[2, 3, 'Paul the Wine Guy']], []], 'emotion': None, 'caption': None, 'transcript_with_note': None, 'tokens_with_note': None}))\n",
            "Utterance(id: 's01_e01_c01_u004', conversation_id: s01_e01_c01_u001, reply-to: s01_e01_c01_u003, speaker: Speaker(id: 'Phoebe Buffay', vectors: [], meta: ConvoKitMeta({})), timestamp: None, text: 'Wait, does he eat chalk?', vectors: [], meta: ConvoKitMeta({'tokens': [['Wait', ',', 'does', 'he', 'eat', 'chalk', '?']], 'character_entities': [[[3, 4, 'Paul the Wine Guy']]], 'emotion': None, 'caption': None, 'transcript_with_note': None, 'tokens_with_note': None}))\n",
            "Utterance(id: 's01_e01_c01_u005', conversation_id: s01_e01_c01_u001, reply-to: s01_e01_c01_u004, speaker: Speaker(id: 'TRANSCRIPT_NOTE', vectors: [], meta: ConvoKitMeta({})), timestamp: None, text: '', vectors: [], meta: ConvoKitMeta({'tokens': [], 'character_entities': [], 'emotion': None, 'caption': None, 'transcript_with_note': '(They all stare, bemused.)', 'tokens_with_note': [['(', 'They', 'all', 'stare', ',', 'bemused', '.', ')']]}))\n",
            "Utterance(id: 's01_e01_c01_u006', conversation_id: s01_e01_c01_u001, reply-to: s01_e01_c01_u005, speaker: Speaker(id: 'Phoebe Buffay', vectors: [], meta: ConvoKitMeta({})), timestamp: None, text: \"Just, 'cause, I don't want her to go through what I went through with Carl- oh!\", vectors: [], meta: ConvoKitMeta({'tokens': [['Just', ',', \"'\", 'cause', ',', 'I', 'do', \"n't\", 'want', 'her', 'to', 'go', 'through', 'what', 'I', 'went', 'through', 'with', 'Carl', '-', 'oh', '!']], 'character_entities': [[[5, 6, 'Phoebe Buffay'], [9, 10, 'Monica Geller'], [14, 15, 'Phoebe Buffay'], [18, 19, \"Carl (Rachel's date)\"]]], 'emotion': None, 'caption': None, 'transcript_with_note': None, 'tokens_with_note': None}))\n",
            "Utterance(id: 's01_e01_c01_u007', conversation_id: s01_e01_c01_u001, reply-to: s01_e01_c01_u006, speaker: Speaker(id: 'Monica Geller', vectors: [], meta: ConvoKitMeta({})), timestamp: None, text: \"Okay, everybody relax. This is not even a date. It's just two people going out to dinner and- not having sex.\", vectors: [], meta: ConvoKitMeta({'tokens': [['Okay', ',', 'everybody', 'relax', '.'], ['This', 'is', 'not', 'even', 'a', 'date', '.'], ['It', \"'s\", 'just', 'two', 'people', 'going', 'out', 'to', 'dinner', 'and', '-', 'not', 'having', 'sex', '.']], 'character_entities': [[], [], []], 'emotion': None, 'caption': None, 'transcript_with_note': None, 'tokens_with_note': None}))\n",
            "Utterance(id: 's01_e01_c01_u008', conversation_id: s01_e01_c01_u001, reply-to: s01_e01_c01_u007, speaker: Speaker(id: 'Chandler Bing', vectors: [], meta: ConvoKitMeta({})), timestamp: None, text: 'Sounds like a date to me.', vectors: [], meta: ConvoKitMeta({'tokens': [['Sounds', 'like', 'a', 'date', 'to', 'me', '.']], 'character_entities': [[[5, 6, 'Chandler Bing']]], 'emotion': None, 'caption': None, 'transcript_with_note': None, 'tokens_with_note': None}))\n",
            "Utterance(id: 's01_e01_c01_u009', conversation_id: s01_e01_c01_u001, reply-to: s01_e01_c01_u008, speaker: Speaker(id: 'TRANSCRIPT_NOTE', vectors: [], meta: ConvoKitMeta({})), timestamp: None, text: '', vectors: [], meta: ConvoKitMeta({'tokens': [], 'character_entities': [], 'emotion': None, 'caption': None, 'transcript_with_note': '[Time Lapse]', 'tokens_with_note': [['[', 'Time', 'Lapse', ']']]}))\n",
            "Utterance(id: 's01_e01_c01_u010', conversation_id: s01_e01_c01_u001, reply-to: s01_e01_c01_u009, speaker: Speaker(id: 'Chandler Bing', vectors: [], meta: ConvoKitMeta({})), timestamp: None, text: \"Alright, so I'm back in high school, I'm standing in the middle of the cafeteria, and I realize I am totally naked.\", vectors: [], meta: ConvoKitMeta({'tokens': [['Alright', ',', 'so', 'I', \"'m\", 'back', 'in', 'high', 'school', ',', 'I', \"'m\", 'standing', 'in', 'the', 'middle', 'of', 'the', 'cafeteria', ',', 'and', 'I', 'realize', 'I', 'am', 'totally', 'naked', '.']], 'character_entities': [[[3, 4, 'Chandler Bing'], [10, 11, 'Chandler Bing'], [21, 22, 'Chandler Bing'], [23, 24, 'Chandler Bing']]], 'emotion': None, 'caption': None, 'transcript_with_note': None, 'tokens_with_note': None}))\n"
          ]
        }
      ],
      "source": [
        "for i, utterance in enumerate(corpus.iter_utterances()):\n",
        "    print(utterance)\n",
        "    if (i >= 9):\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "713e7564",
      "metadata": {
        "id": "713e7564"
      },
      "source": [
        "**Graded Task**: Create a list of strings called `uts` that contains\n",
        "all utterances made by your favourite (of the 6) main character of the show.\n",
        "\n",
        "If you need help, check the documentation [https://convokit.cornell.edu/documentation/]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf0cd046",
      "metadata": {
        "id": "cf0cd046"
      },
      "outputs": [],
      "source": [
        "character = 'Monica Geller' # OR 'Chandler Bing' OR 'Phoebe Buffay' OR ...\n",
        "# pass # TODO\n",
        "uts = []\n",
        "for i, utterance in enumerate(corpus.iter_utterances()):\n",
        "    if utterance.speaker.id == character:\n",
        "        uts.append(utterance.text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c493e86e",
      "metadata": {
        "id": "c493e86e"
      },
      "source": [
        "Please include the output of this next line in your solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cce37f65",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cce37f65",
        "outputId": "6d122886-b0aa-43f2-a95d-3d0e10ff9b8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8498\n"
          ]
        }
      ],
      "source": [
        "print(len(uts))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d853b29",
      "metadata": {
        "id": "6d853b29"
      },
      "source": [
        "**Task**: Run the below code. This code combines these lines into a\n",
        "large string for training, and a large string for validation.\n",
        "We will index ranges in this large string for use in a minibatch---i.e.\n",
        "a minibatch of data will consist of a substring in this large string.\n",
        "This substring may start in the middle of an utterance and may contain\n",
        "multiple utterances---and that turns out to be okay!\n",
        "Our neural network still manages to learn what an utterance looks\n",
        "like and emulate it.\n",
        "\n",
        "Since this approach is simpler to implement than the batching approach\n",
        "seen in the previous lab, it is more often used.\n",
        "\n",
        "We will use 90% of the data for training, and 10% of the data\n",
        "for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5bf9073",
      "metadata": {
        "id": "c5bf9073"
      },
      "outputs": [],
      "source": [
        "train_split = 0.9\n",
        "n = len(uts)\n",
        "\n",
        "train_data_str = '\\n'.join(uts[:int(n*train_split)])\n",
        "val_data_str = '\\n'.join(uts[int(n*train_split):])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d325b81",
      "metadata": {
        "id": "8d325b81"
      },
      "source": [
        "**Task**: Notice that we split the utterances so that the earlier utterances\n",
        "are in the training set, and the later utterances (i.e., later in the TV show)\n",
        "are in the validation set. Why is this method preferable to randomly splitting\n",
        "the utterances into training and validation?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4367f40d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "4367f40d",
        "outputId": "27dc9ebb-88a7-47e0-a7f1-883cd2268a67"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe utterances are likely to be sequential. We split this way because we want\\nour model to be able to predict the sequential content.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Include your explanation here\n",
        "# print(uts[:3])\n",
        "# print(train_data_str[:100])\n",
        "\"\"\"\n",
        "The utterances are likely to be sequential. We split this way because we want\n",
        "our model to be able to predict the sequential content.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "319b0669",
      "metadata": {
        "id": "319b0669"
      },
      "source": [
        "** **bold text**Task**: Why do we not set aside a test set?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd1a60f3",
      "metadata": {
        "id": "dd1a60f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "cb9f7f01-582f-4e73-aefd-184ecb5cad92"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nSince the model is generative using unsupervised learning technique, we dont\\nacutally need a test set to evalurate the performance.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Include your explanation here\n",
        "\"\"\"\n",
        "Since the model is generative using unsupervised learning technique, we dont\n",
        "acutally need a test set to evalurate the performance.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6742ae32",
      "metadata": {
        "id": "6742ae32"
      },
      "source": [
        "Just like in the previous lab, we will *tokenize* our text.\n",
        "Modern models use a tokenization strategy called\n",
        "[Byte Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding),\n",
        "which tokenize text into common into **sub-word tokens**.\n",
        "Sub-word tokens split words into commonly occuring (and thus meaningful) parts.\n",
        "For example, the word \"utterance\" could be split\n",
        "into \"utter\" and \"ance\". The model would learn about these constinuent\n",
        "tokens in different contexts, helping the model generalize better.\n",
        "\n",
        "We will use the Byte Pair Encoding (BPE) tokenizer from the\n",
        "`tiktoken` library. Let's install and import this library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "820b7bc7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "820b7bc7",
        "outputId": "f2a2812d-20d3-473b-b8db-17730ca99a7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ],
      "source": [
        "%pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf2c64e5",
      "metadata": {
        "id": "bf2c64e5"
      },
      "outputs": [],
      "source": [
        "import tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "612b18d6",
      "metadata": {
        "id": "612b18d6"
      },
      "source": [
        "**Graded Task**: We will be fine-tuning a pre-trained GPT2 model.\n",
        "Explain why it is important for us to use the same tokenization\n",
        "method as is used in the original GPT2 model whose weights we will be\n",
        "using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d01a5788",
      "metadata": {
        "id": "d01a5788",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "c41ac458-fa56-4134-b9a9-9075993feac7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nIf we use different tokenization methods, the weights of GPT model could decode a\\nwords to another words. For example, if we tokenize 'goat' to <token>, the tokenizer\\nGPT2 model uses might intepret <token> as 'UofT'.\\nAs a result, the weight of GPT2 model will not act properly and won't help us\\ntrain the model.\\nHence it is important to use the same tokenizer.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Your explanation goes here\n",
        "\"\"\"\n",
        "If we use different tokenization methods, the weights of GPT model could decode a\n",
        "words to another words. For example, if we tokenize 'goat' to <token>, the tokenizer\n",
        "GPT2 model uses might intepret <token> as 'UofT'.\n",
        "As a result, the weight of GPT2 model will not act properly and won't help us\n",
        "train the model.\n",
        "Hence it is important to use the same tokenizer.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef4d4648",
      "metadata": {
        "id": "ef4d4648"
      },
      "source": [
        "Now, let's retrieve the original GPT2 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "674edc60",
      "metadata": {
        "id": "674edc60"
      },
      "outputs": [],
      "source": [
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "train_ids = enc.encode_ordinary(train_data_str)\n",
        "val_ids = enc.encode_ordinary(val_data_str)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore this cell\n",
        "trs = train_data_str[:104]\n",
        "print(trs)\n",
        "print(trs.split())\n",
        "print(len(trs.split()))\n",
        "trid = enc.encode_ordinary(trs)\n",
        "print(trid)\n",
        "print(len(trid))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35t6zvWq-ZuJ",
        "outputId": "6c512bb5-ec52-474a-d68c-5c1f22ffdcac"
      },
      "id": "35t6zvWq-ZuJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There's nothing to tell! He's just some guy I work with!\n",
            "Okay, everybody relax. This is not even a date.\n",
            "[\"There's\", 'nothing', 'to', 'tell!', \"He's\", 'just', 'some', 'guy', 'I', 'work', 'with!', 'Okay,', 'everybody', 'relax.', 'This', 'is', 'not', 'even', 'a', 'date.']\n",
            "20\n",
            "[1858, 338, 2147, 284, 1560, 0, 679, 338, 655, 617, 3516, 314, 670, 351, 0, 198, 16454, 11, 7288, 8960, 13, 770, 318, 407, 772, 257, 3128, 13]\n",
            "28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dc3a7fe",
      "metadata": {
        "id": "9dc3a7fe"
      },
      "source": [
        "**Task**: How many tokens are in the training and validation sets?\n",
        "How does this compare with the number of *words* in each data set\n",
        "(computed using the `str.split()` method)?\n",
        "What about the number of *characters*?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85f3940f",
      "metadata": {
        "id": "85f3940f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54899c1f-ceee-449a-f6a4-68c553df150c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# tokens in training set: 112149\n",
            "# tokens in validation set: 12572\n",
            "# words in training set: 74572\n",
            "# words in validation set: 8527\n",
            "# char in training set: 388894\n",
            "# char in validation set: 44133\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "print(f'# tokens in training set: {len(train_ids)}')\n",
        "print(f'# tokens in validation set: {len(val_ids)}')\n",
        "print(f'# words in training set: {len(train_data_str.split())}')\n",
        "print(f'# words in validation set: {len(val_data_str.split())}')\n",
        "print(f'# char in training set: {len(train_data_str)}')\n",
        "print(f'# char in validation set: {len(val_data_str)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67afc519",
      "metadata": {
        "id": "67afc519"
      },
      "source": [
        "**Task**: Run the below code, which will save the above numpy arrays in a file.\n",
        "This way, we can use the `np.memmap` function, which creates a memory-map\n",
        "to an array stored in a binary file on disk. This approach is useful for\n",
        "accessing segments of a large file on disk, which we will be doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89dd57d5",
      "metadata": {
        "id": "89dd57d5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "data_dir = 'friends_gpt2'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "# export to bin files\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile(os.path.join(data_dir, 'train.bin'))\n",
        "val_ids.tofile(os.path.join(data_dir, 'val.bin'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e602cd2c",
      "metadata": {
        "id": "e602cd2c"
      },
      "outputs": [],
      "source": [
        "# create a memory map\n",
        "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8164cef",
      "metadata": {
        "id": "b8164cef"
      },
      "source": [
        "**Task**: Use the `get_batch` function below to extract a sample input/output\n",
        "from this data set. Here, we will be using the approach shown in\n",
        "the generative RNN lecture, where the model generates the next token given the\n",
        "previous context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f53ad0a",
      "metadata": {
        "id": "1f53ad0a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def get_batch(data, block_size, batch_size, device):\n",
        "    \"\"\"\n",
        "    Return a minibatch of data. This function is not deterministic.\n",
        "    Calling this function multiple times will result in multiple different\n",
        "    return values.\n",
        "\n",
        "    Parameters:\n",
        "        `data` - a numpy array (e.g., created via a call to np.memmap)\n",
        "        `block_size` - the length of each sequence\n",
        "        `batch_size` - the number of sequences in the batch\n",
        "        `device` - the device to place the returned PyTorch tensor\n",
        "\n",
        "    Returns: A tuple of PyTorch tensors (x, t), where\n",
        "        `x` - represents the input tokens, with shape (batch_size, block_size)\n",
        "        `y` - represents the target output tokens, with shape (batch_size, block_size)\n",
        "    \"\"\"\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    t = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if 'cuda' in device:\n",
        "        # pin arrays x,t, which allows us to move them to GPU asynchronously\n",
        "        #  (non_blocking=True)\n",
        "        x, t = x.pin_memory().to(device, non_blocking=True), t.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, t = x.to(device), t.to(device)\n",
        "    return x, t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6deb9080",
      "metadata": {
        "id": "6deb9080",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8f4e6a5-0985-45c2-efea-99e0d494cbf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[  670,   329,   262,  ...,    13,   198, 44045],\n",
            "        [  318,   326,   318,  ...,   314,  1549,   307],\n",
            "        [ 1310,  4141,    13,  ...,    11,   314,   655],\n",
            "        [16829,   262, 21221,  ...,   290,    12,   392],\n",
            "        [  612,     0,   198,  ...,   750,   530,   640]]), tensor([[  329,   262,  4436,  ...,   198, 44045,    11],\n",
            "        [  326,   318,   373,  ...,  1549,   307,  3501],\n",
            "        [ 4141,    13,   569,  ...,   314,   655, 19169],\n",
            "        [  262, 21221,    13,  ...,    12,   392,  5875],\n",
            "        [    0,   198,  5812,  ...,   530,   640,    11]]))\n",
            "2 torch.Size([5, 1024]) torch.Size([5, 1024])\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available()  else 'cpu'\n",
        "# TODO: get and print a single batch from the training set\n",
        "batch = get_batch(train_data, 1024, 5, device)\n",
        "print(batch)\n",
        "print(len(batch), batch[0].shape, batch[1].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7aa876f",
      "metadata": {
        "id": "c7aa876f"
      },
      "source": [
        "**Graded Task**: Once again, we will be using the approach shown in the generative\n",
        "RNN lecture, where the model's goal is to generate the next token given the\n",
        "previous context. With that in mind, explain why the target output tokens is\n",
        "very similar to the input tokens, just offset by 1 along the `block_size`\n",
        "dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6751a1ad",
      "metadata": {
        "id": "6751a1ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "508baeff-f431-4719-d608-6475dbf5a08b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe offset is only 1 since we are predicting next token from the input and append the result to output.\\nAfter passing the current output to the next iteration as the input, we also need \\nto maintain the original sequence length for our model to function so that the input \\nsize is not changed.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# TODO: Your explanation goes here.\n",
        "\"\"\"\n",
        "The offset is only 1 since we are predicting next token from the input and append the result to output.\n",
        "After passing the current output to the next iteration as the input, we also need\n",
        "to maintain the original sequence length for our model to function so that the input\n",
        "size is not changed.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a56959bd",
      "metadata": {
        "id": "a56959bd"
      },
      "source": [
        "## Part 2. Model\n",
        "\n",
        "Now that we have our data set in mind, it is time to set up our GPT2 model.\n",
        "We will use the code provided in the [nanoGPT repository](https://github.com/karpathy/nanoGPT),\n",
        "slightly modified here for succinctness.\n",
        "Thus, we will not re-implement the GPT2 model. Instead, let's use the\n",
        "nanoGPT implementation to understand, step-by-step, what happens in a GPT model.\n",
        "\n",
        "\n",
        "We will explore the components of the GPT2 model first in a *top-down* manner,\n",
        "to get an intuition as to how the pieces connect. Then, we will explore the same\n",
        "components in a *bottom-up* manner, so that we can fully understand the role of each\n",
        "component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "059d1376",
      "metadata": {
        "id": "059d1376"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import inspect"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db98056d",
      "metadata": {
        "id": "db98056d"
      },
      "source": [
        "We begin with the `GPTConfig` class, which contains model architecture\n",
        "settings for our GPT2 model. The settings specify:\n",
        "\n",
        "- `block_size`: the input sequence length. Shorter sequences can be padded (with\n",
        "  a padding token as seen in the previous lab), and\n",
        "  longer sequences must be cut shorter. During training, we will generate batches\n",
        "  with sequences that are exactly the block size\n",
        "- `vocab_size`: the number of unique tokens in our vocabulary. This affects the\n",
        "  size of the initial embedding layer.\n",
        "- `n_layer`: the number of transformer layers.\n",
        "- `n_head`: the number of *attention heads* to use in the causal self-attention layer.\n",
        "- `n_embd`: the embedding size used throughout the model. You can think of each token\n",
        "  position as being represented as a vector of length `n_embd`.\n",
        "- `dropout`: for dropout.\n",
        "- `bias`: A boolean to determine whether to use a bias parameter in certain layers or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6d9ed03",
      "metadata": {
        "id": "a6d9ed03"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3d81292",
      "metadata": {
        "id": "d3d81292"
      },
      "source": [
        "**Task**: Which of these settings do you think would affect the total number of trainable parameters in a GPT model?\n",
        "Which of them do you think have the **largest impact** on the number of trainable parameters?\n",
        "Please write down your guess before continuing, and we will check back here later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47095cd7",
      "metadata": {
        "id": "47095cd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "56f46b83-16fe-47e3-ad2d-8d44d46aab4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# trainable param: block size, vocab size, n_layer, n_head, n_embd\\nn-layers would have the largest impact\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# TODO: Write down your thoughts here.\n",
        "\"\"\"\n",
        "# trainable param: block size, vocab size, n_layer, n_head, n_embd\n",
        "n-layers would have the largest impact\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2f3d46d",
      "metadata": {
        "id": "a2f3d46d"
      },
      "source": [
        "With the setting in mind, we can set up a GPT model.\n",
        "A GPT model will take a GPTConfig object as a parameter.\n",
        "Pay particular attention to the `__init__()` and\n",
        "`forward()` methods. These are the methods that we will\n",
        "study in more detail.\n",
        "\n",
        "The code uses a more PyTorch features that we have not discussed,\n",
        "but these features are mostly cosmetic and do not provide significantly\n",
        "different functionality: the use of `nn.ModuleDict` allows us to access modules\n",
        "in the `GPT` class in a straightforward way, and\n",
        "`nn.ModuleList` allows us to create a list of modules.\n",
        "We have not yet defined the PyTorch neural network modules `Block` and\n",
        "`LayerNorm`, but we will do so soon.\n",
        "\n",
        "If you see a PyTorch feature used that you don't understand, you can\n",
        "always look it up in the PyTorch documentation. However, you don't try to\n",
        "understand everything at one go. It is normal to read code in multiple \"passes\",\n",
        "and focus on the big picture in the first pass.\n",
        "\n",
        "**Task**: Begin with a first pass read of the `__init__()`\n",
        "and `forward()` methods of the `GPT` module.\n",
        "\n",
        "**Graded Task**: Implement the `generate` method of the class. This method should autoregressively generate the next token ids based on the given tokens. Implementation details and type annotations are provided in the method. Make sure to verify your solution with a sensible test of your design. Think about possible edge cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a272ac5d",
      "metadata": {
        "id": "a272ac5d"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        # with weight tying when using torch.compile() some warnings get generated:\n",
        "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
        "        # This behavior is deprecated and will be an error in future versions\"\n",
        "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
        "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def crop_block_size(self, block_size):\n",
        "        # model surgery to decrease the block size if necessary\n",
        "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
        "        # but want to use a smaller block size for some smaller, simpler model\n",
        "        assert block_size <= self.config.block_size\n",
        "        self.config.block_size = block_size\n",
        "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
        "        for block in self.transformer.h:\n",
        "            if hasattr(block.attn, 'bias'):\n",
        "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type, override_args=None):\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        override_args = override_args or {} # default to empty dict\n",
        "        # only dropout can be overridden see more notes below\n",
        "        assert all(k == 'dropout' for k in override_args)\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        config_args['bias'] = True # always True for GPT model checkpoints\n",
        "        # we can override the dropout rate, if desired\n",
        "        if 'dropout' in override_args:\n",
        "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
        "            config_args['dropout'] = override_args['dropout']\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
        "        # first estimate the number of flops we do per iteration.\n",
        "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
        "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx: torch.Tensor, new_tokens: int, temperature: float=1.0, top_k: int|None=None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Make sure to account for the fact that input sequences or intermediate generation might\n",
        "        be longer than the max block size, you have to process those (yor code cannot generate\n",
        "        an error in this case).\n",
        "\n",
        "        Args:\n",
        "            idx (torch.Tensor): a tensor of tokenized inputs of shape (b,t) where b is a batch\n",
        "            and t is the time axis.\n",
        "            new_tokens (int): the number of tokens to return\n",
        "            temperature (float): the sampling temperature for the softmax operation\n",
        "\n",
        "        Return:\n",
        "            torch.Tensor: a tensor of autoregressively sampled next ids\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        for _ in range(new_tokens):\n",
        "            if idx.shape[1] > self.config.block_size:\n",
        "                idx = idx[:, -self.config.block_size:]\n",
        "            logits, loss = self(idx)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k:\n",
        "                ilow_k = torch.argsort(logits)[top_k:]\n",
        "                logits[ilow_k] = float('-inf')\n",
        "\n",
        "            prob = F.softmax(logits, dim=-1)\n",
        "            token = torch.multinomial(prob, 1)\n",
        "            # print(idx, idx.shape)\n",
        "            # print(prob, prob.shape)\n",
        "            # print('token', token)\n",
        "            idx = torch.cat((idx, token), dim=1)\n",
        "            # print(idx)\n",
        "\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d785319d",
      "metadata": {
        "id": "d785319d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "2e13d6e9-6d55-4d22-b2f7-dc290d995705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 123.69M\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-5d09b7b1bcbf>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO: test code here (this will only run after the Blocks are defined further below, sorry about that)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGPTConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-5dcabe0615c7>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, idx, new_tokens, temperature, top_k)\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-5dcabe0615c7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok_emb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-ad6c94135521>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-ac8a169f4132>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflash\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# efficient attention using Flash Attention CUDA kernels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# manual implementation of attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# TODO: test code here (this will only run after the Blocks are defined further below, sorry about that)\n",
        "model = GPT(GPTConfig()).to(device)\n",
        "model.generate(batch[0], 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afb5b193",
      "metadata": {
        "id": "afb5b193"
      },
      "source": [
        "Doing a first-pass read on both the `__init__()` and `forward()` methods,\n",
        "you should see that the GPT model has the following components (ignoring\n",
        "dropout):\n",
        "\n",
        "- `transformer.wte`, which is an embedding layer that maps tokens\n",
        "  to a vector embedding.\n",
        "- `transformer.wtp`, is also an embedding layer, but this one maps\n",
        "  **token position indices** to a vector embedding. This index is required to\n",
        "  inject position information into the embedding---otherwise transformer computation\n",
        "  would be invariant to the reordering of input tokens (i.e., the computation would not\n",
        "  change if the order of the input tokens change).\n",
        "  Since the length of a sequence is at most `block_size`, so there\n",
        "  are at most `block_size` indices to embed.\n",
        "- A sequence of `Block`s --- to be defined. The output from the previous block\n",
        "  is taken as the input of the next block.\n",
        "- A final `LayerNorm` layer after the last block. This layer is also yet to be\n",
        "  defined, and the name suggests that this is a normalization layer (similar to\n",
        "  batch normalization) that does *not* change the shape of the features\n",
        "  (i.e., the output shape is the same as the input shape).\n",
        "- `lm_head`, which is a linear layer that maps embeddings back to a distribution\n",
        "  over the possible otuput tokens.\n",
        "\n",
        "**Task**: Compute the number of parameters in the `wte` embedding layer of the GPT2 model. (For these and other questions that specifically mention GPT2 model,\n",
        "please use the config settings above and provide an actual numbers.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66cfdccc",
      "metadata": {
        "id": "66cfdccc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dfa0ecf-32af-4cd5-a41c-d2362cffad08"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# TODO: Perform this computation by hand.\n",
        "\"\"\"\n",
        "vocab_size * n_embd\n",
        "\"\"\"\n",
        "embedding = nn.Embedding(10, 3)\n",
        "# a batch of 2 samples of 4 indices each\n",
        "input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n",
        "embedding(input).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e93a6cd",
      "metadata": {
        "id": "4e93a6cd"
      },
      "source": [
        "**Task**: Compute the number of parameters in the `wtp` embedding layer of the GPT2 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39c6daf2",
      "metadata": {
        "id": "39c6daf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "666f2e4a-adbc-41e9-bfe8-f245bbd04859"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nblock_size * n_embd\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# TODO: Perform this computation by hand.\n",
        "\"\"\"\n",
        "block_size * n_embd\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6b43ff2",
      "metadata": {
        "id": "d6b43ff2"
      },
      "source": [
        "**Graded Task**: Explain why the linear layer `lm_head` has the same number\n",
        "of parameters as the embedding layer `wte`. Provide an intuitive explanation\n",
        "for why **weight tying**---i.e., using the same set of weights for both layers,\n",
        "just transposed---would be reasonable. The weight tying is done to reduce the\n",
        "total number of parameters in the GPT2 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d8f72a0",
      "metadata": {
        "id": "1d8f72a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "5b61f3bf-3cb1-4b7b-dfd1-dee6d55685a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWe are taken in input that need to be processed by wte for the deeper layers.\\nAfter the data travelled down to the final layer, we need to \"translate\" it\\nback to the original form (a matrix of vocabs). That\\'s why we use weight tying.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# TODO: Include your explanations here\n",
        "\"\"\"\n",
        "We are taken in input that need to be processed by wte for the deeper layers.\n",
        "After the data travelled down to the final layer, we need to \"translate\" it\n",
        "back to the original form (a matrix of vocabs). That's why we use weight tying.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "559c8e0b",
      "metadata": {
        "id": "559c8e0b"
      },
      "source": [
        "**Task**: Explain why it is that in the `forward()` method, the tensor `tok_emb` has\n",
        "the shape `(b, t, n_embd)`, where `b` is the batch size, `t` is the sequence\n",
        "length (max `block_size`), and `n_embd` is the embedding size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eeb74156",
      "metadata": {
        "id": "eeb74156",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "12bb4e70-1950-4ae2-b021-cedc78d20e86"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n(b, t) is the size of idx. wte update each token at each time to a (n-embd) vector,\\nresulting in a tensor of size (b, t, n_embd)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# TODO: Include your explanations here\n",
        "\"\"\"\n",
        "(b, t) is the size of idx. wte update each token at each time to a (n-embd) vector,\n",
        "resulting in a tensor of size (b, t, n_embd)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6a4dc23",
      "metadata": {
        "id": "a6a4dc23"
      },
      "source": [
        "**Task**: Notice that in the `forward()` method, the tensor `pos_emb` has\n",
        "the shape `(t, n_embd)`. In other words, we embed the position only once for\n",
        "each batch, and then rely on PyTorch tensor broadcasting to perform the\n",
        "addition `tok_emb + pos_emb`. Why is this ok?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acb5fbb8",
      "metadata": {
        "id": "acb5fbb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "28fdf165-13f0-4c69-91fb-eeb61e1d3964"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWe are sharing the token position indices to vector embedding mapping across the\\nwhole batch. This is okay since each token should have similar mapping. This\\napproach shrinks the parameter size down by <batch_size> times.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# TODO: Include your explanations here\n",
        "\"\"\"\n",
        "We are sharing the token position indices to vector embedding mapping across the\n",
        "whole batch. This is okay since each token should have similar mapping. This\n",
        "approach shrinks the parameter size down by <batch_size> times.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e2b5f8b",
      "metadata": {
        "id": "8e2b5f8b"
      },
      "source": [
        "**Task**: What is the shape of `tok_emb + pos_emb`  in the `forward()` method\n",
        "in a GPT2 model?\n",
        "This question is not trivial because the two addend tensors are not of the\n",
        "same shape.\n",
        "Thus, the addition uses broadcasting. PyTorch broadcasting works similarly to\n",
        "that of Numpy's.  You can look up \"PyTorch broadcasting\" to find resources\n",
        "related to how broadcasting works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dffa9df1",
      "metadata": {
        "id": "dffa9df1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c49421a8-1309-4afa-eed4-0896f524f3ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n(b, t, n_embd)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "# TODO: Perform this computation by hand.\n",
        "\"\"\"\n",
        "(b, t, n_embd)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db099778",
      "metadata": {
        "id": "db099778"
      },
      "source": [
        "**Graded Task**: What is the shape of `x`  in the `forward()` method? This is an\n",
        "important shape to remember, since it is the shape of the feature map consistent\n",
        "in most of the transformer network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b173b904",
      "metadata": {
        "id": "b173b904",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "6ad93a9d-4f89-468b-8f53-09e6a302ce41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n(b, t, n_embd). Since dropout layer doesn't change the dimension of the input.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# TODO: Perform this computation by hand.\n",
        "\"\"\"\n",
        "(b, t, n_embd). Since dropout layer doesn't change the dimension of the input.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07b4510f",
      "metadata": {
        "id": "07b4510f"
      },
      "source": [
        "**Task**: What is the shape of `logits`  in the `forward()` method?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8c96349",
      "metadata": {
        "id": "c8c96349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "963e69be-9678-4927-f7a3-3aaf8a3a2dfc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n(batch size, vocab_size)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# TODO: Perform this computation by hand.\n",
        "\"\"\"\n",
        "(batch size, vocab_size)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15cabec9",
      "metadata": {
        "id": "15cabec9"
      },
      "source": [
        "These questions above should give you a clear idea of the main components\n",
        "of the transformer model, the expected input and output tensor shapes, and\n",
        "the shapes of intermediate tensors. With this in mind, let's explore\n",
        "the two modules referenced by `GPT`.\n",
        "\n",
        "We'll start with the simple one. The LayerNorm layer is intended to be\n",
        "similar to [PyTorch's LayerNorm layer](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2d10dd9",
      "metadata": {
        "id": "b2d10dd9"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f96a22c8",
      "metadata": {
        "id": "f96a22c8"
      },
      "source": [
        "**Task**: How many parameters are in a LayerNorm layer?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a85264e",
      "metadata": {
        "id": "2a85264e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "5badeb9f-8a77-4cee-d135-90e15507ec5f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nweight: ndim\\nbias: ndim if defined else 0\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# TODO: Perform this computation by hand.\n",
        "\"\"\"\n",
        "weight: ndim\n",
        "bias: ndim if defined else 0\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2821e28f",
      "metadata": {
        "id": "2821e28f"
      },
      "source": [
        "**Task**: Read the description of the LayerNorm layer in PyTorch at\n",
        "[https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm). Then, explain how layer normalization differs from batch normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78581d7d",
      "metadata": {
        "id": "78581d7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a476939e-2449-4e6a-bce1-23dc36a040e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nLayerNorm operation is independent of the batch while BatchNorm is not\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# TODO: Perform this computation by hand.\n",
        "\"\"\"\n",
        "LayerNorm operation is independent of the batch while BatchNorm is not\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f4de174",
      "metadata": {
        "id": "1f4de174"
      },
      "source": [
        "Let's move on to the `Block` module. Recall that here are several `Block`\n",
        "modules in a GPT model, and the output of one module is the input of the next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29660279",
      "metadata": {
        "id": "29660279"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03aa194a",
      "metadata": {
        "id": "03aa194a"
      },
      "source": [
        "This module is actually quite succinct, but it also refers to modules that are\n",
        "yet to be defined. It consists of:\n",
        "\n",
        "- A layer normalization layer.\n",
        "- A **causal self attention** layer (to be defined). This is the heart of the GPT model.\n",
        "- Another layer normalization layer.\n",
        "- An MLP layer (to be defined).\n",
        "\n",
        "**Task**: Judging by the `Block.forward()` method above, why must\n",
        "the `CausalSelfAttention` and the `MLP` layers\n",
        "**preserve the shape of the features**?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2408be59",
      "metadata": {
        "id": "2408be59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3aa14454-1964-4bf2-ecf3-079fffe5f68b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nThe block's output will be passed to next block.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# TODO: Include your explanation here.\n",
        "\"\"\"\n",
        "The block's output will be passed to next block.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "714df529",
      "metadata": {
        "id": "714df529"
      },
      "source": [
        "**Task**: How might the *skip-connections* in the `Block.forward()` method\n",
        "help with gradient flow? An intuitive explanation is sufficient here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c709c541",
      "metadata": {
        "id": "c709c541",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "e6f3b22e-4c63-4cc3-cd7f-d96bbb1c0b35"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIt provides a generally stable gradient signal even when the gradient of the\\nlatter term is minimal since gradient of x is 1, so it will not vanish\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# TODO: Include your explanation here.\n",
        "\"\"\"\n",
        "It provides a generally stable gradient signal even when the gradient of the\n",
        "latter term is minimal since gradient of x is 1, so it will not vanish\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d00cd112",
      "metadata": {
        "id": "d00cd112"
      },
      "source": [
        "With the GPT2 `Block` in mind, we will define the `MLP` module next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "811b239a",
      "metadata": {
        "id": "811b239a"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6d1d83a",
      "metadata": {
        "id": "c6d1d83a"
      },
      "source": [
        "Immediately, we see that this MLP consists of two linear layers.\n",
        "The activation function used between these two layers is the\n",
        "[Gaussian Error Linear Units function](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html). You can read more about it in\n",
        "[this paper](https://arxiv.org/pdf/1606.08415.pdf).\n",
        "\n",
        "**Task**: Compute the number of parameters in a `MLP` layer in a GPT2 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42ff1aa1",
      "metadata": {
        "id": "42ff1aa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "47f2318d-6a19-447d-d95a-717c3c797771"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n4 * config.n_embd * config.n_embd * 2\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "# TODO: Perform this computation by hand\n",
        "\"\"\"\n",
        "4 * config.n_embd * config.n_embd * 2\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68f1d219",
      "metadata": {
        "id": "68f1d219"
      },
      "source": [
        "**Task**: Recall that the input of the MLP layer is a tensor with the usual\n",
        "dimension computed earlier.\n",
        "What is the shape of `self.c_fc(x)` in the `MLP.forward()` method?\n",
        "What about the shape of the return value in this method?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e7a74d4",
      "metadata": {
        "id": "3e7a74d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "5d77f9cd-dfc4-4a0f-823d-09cfb9c2d2e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n(b, t, n_embd * 4)\\n(b, t, n_embd)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "# TODO: Perform this computation by hand\n",
        "\"\"\"\n",
        "(b, t, n_embd * 4)\n",
        "(b, t, n_embd)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e30b2e8",
      "metadata": {
        "id": "8e30b2e8"
      },
      "source": [
        "**Task**: Explain why this MLP layer is also called the \"pointwise feed forward\"\n",
        "layer.  (Hint: a \"point\" here refers to a single token or position in the input\n",
        "sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd9df9dd",
      "metadata": {
        "id": "cd9df9dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "201850f5-725a-45be-e65e-47fa692d5f2f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nBecause this MLP is applied point-wise to all tokens in a batch independently.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# TODO: Include your explanation here.\n",
        "\"\"\"\n",
        "Because this MLP is applied point-wise to all tokens in a batch independently.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93d9f81c",
      "metadata": {
        "id": "93d9f81c"
      },
      "source": [
        "Finally, let's study the definition of the `CausalSelfAttention` layer.\n",
        "This is the heart of the GPT model and is also the most complex module.\n",
        "\n",
        "**Task**: Begin with a first pass read of the `__init__()`\n",
        "and `forward()` methods of `CausalSelfAttention` module.\n",
        "We will then trace through the case where `self.flash` is `False`, since\n",
        "the code provides more detailed explanation for the computation steps.\n",
        "\n",
        "**Graded Taks**: Fill in the missing `else` block in the `forward` method. This computes attention by hand. Verify your solution!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4e1914c",
      "metadata": {
        "id": "b4e1914c"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        if self.flash:\n",
        "            # efficient attention using Flash Attention CUDA kernels\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            # manual implementation of attention\n",
        "            # TODO\n",
        "            L, S = q.size(-2), k.size(-2)\n",
        "            scale_factor = 1 / math.sqrt(q.size(-1))\n",
        "            attn_bias = torch.zeros(L, S, dtype=q.dtype)\n",
        "            attn_bias.masked_fill_(self.bias.logical_not(), float(\"-inf\"))\n",
        "            attn_bias.to(q.dtype)\n",
        "\n",
        "            attn_weight = q @ k.transpose(-2, -1) * scale_factor\n",
        "            attn_weight += attn_bias\n",
        "            attn_weight = torch.softmax(attn_weight, dim=-1)\n",
        "            attn_weight = torch.attn_dropout(attn_weight, self.dropout, train=True)\n",
        "            y = attn_weight @ v\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b56ebd78",
      "metadata": {
        "id": "b56ebd78"
      },
      "source": [
        "**Task**: Compute the number of parameters in the `c_attn` layer in a GPT2 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75eab3ef",
      "metadata": {
        "id": "75eab3ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a03b8ab3-c4cd-4cc5-801e-1ee3d476dea0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nconfig.n_embd * 3 * config.n_embd\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "# TODO: Perform this computation by hand\n",
        "\"\"\"\n",
        "config.n_embd * 3 * config.n_embd\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfd218d4",
      "metadata": {
        "id": "bfd218d4"
      },
      "source": [
        "**Task**: Like the comment in the `__init__()` method suggests,\n",
        "we can think of the `c_attn` layer as a combination of three\n",
        "`nn.Linear(config.n_embd, config.n_embd, bias=config.bias)` modules.\n",
        "These three networks projects the input embedding into three parts:\n",
        "`q` (for *query*), `k` (for *key*), and `v` (for *value).\n",
        "\n",
        "What is the shape of `self.c_attn(x)` in the `forward()` method?\n",
        "Use this answer to show that\n",
        "`self.c_attn(x).split(self.n_embd, dim=2)` gives us the\n",
        "same `q, k, v` values had we used three separate networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d47f77e0",
      "metadata": {
        "id": "d47f77e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "5ca0839c-c05e-4897-e227-3eb8ace6d3b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n(B, T, nh * 3, hs) or (batch_size, seq_len, num_head * 3, emb_per_head)\\nSince before we split x across dim=2 and do transpose,\\nthe shape was (B, nh, T, hs).\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "# TODO: Perform the computation by hand, then include your explaination.\n",
        "\"\"\"\n",
        "(B, T, nh * 3, hs) or (batch_size, seq_len, num_head * 3, emb_per_head)\n",
        "Since before we split x across dim=2 and do transpose,\n",
        "the shape was (B, nh, T, hs).\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ab48946",
      "metadata": {
        "id": "1ab48946"
      },
      "source": [
        "**Task**: Explain why `config.n_head` must be a factor of `config.n_embd`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cec6cae",
      "metadata": {
        "id": "8cec6cae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d27c3442-0cb4-4405-f457-35457cd0fad2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEnsures that each head receives an equal portion of the embedding space to work with.\\nWe split the embedding to 2 dimensions: head and emb of each head.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# TODO: Your explanation goes here.\n",
        "\"\"\"\n",
        "Ensures that each head receives an equal portion of the embedding space to work with.\n",
        "We split the embedding to 2 dimensions: head and emb of each head.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b330aacf",
      "metadata": {
        "id": "b330aacf"
      },
      "source": [
        "We will explore the manual implementation of attention in the next few\n",
        "questions. For this part, it helps to first consider the\n",
        "case where the batch size `B=1`, and `n_head=1`. For a larger batch size\n",
        "and number of heads, the attention computation is repeated for every\n",
        "sequence in the batch and every attention head. Thus, the shapes of\n",
        "the three important tensors are:\n",
        "\n",
        "- `q`: (1, 1, T, n_embd)\n",
        "- `k`: (1, 1, T, n_embd)\n",
        "- `v`: (1, 1, T, n_embd)\n",
        "\n",
        "Like discussed in the lectures, you can think of the attention mechanism\n",
        "as a \"soft\" dictionary lookup. Instead of obtaining a single key/values for\n",
        "a given query,\n",
        "attention gives us a *probability distribution over the possible keys/values*.\n",
        "We can then use this probability distribution to obtain a weighted sum\n",
        "(akin to an expected value) of the lookup value.\n",
        "Moreover, instead of having strings, numbers, or other objects as keys/values,\n",
        "a key is a vector (of shape `n_embd`), and a value is also a vector\n",
        "(of shape `n_embd`). This is consistent with what we have seen in\n",
        "neural networks---everything is represented using a vector!\n",
        "The tensors `k` and `v` contains these keys and values, and there is one\n",
        "vector at every token position. The tensor `q` contains the **queries**---\n",
        "analogues to the item (a possible key) that we are searching for in a regular\n",
        "dictionary lookup. There is also one query vector for each token position:\n",
        "for each token position, we want to look up a corresponding (weighted sum of) values\n",
        "that contains information pertinent to understanding the meaning of the token\n",
        "in this position.\n",
        "\n",
        "With that in mind, let's go through the mathematical computations.\n",
        "\n",
        "**Graded Task**: What is the shape of `(q @ k.transpose(-2, -1))`?\n",
        "For this and the following questions, assume that `q`, `k`, `v` have the\n",
        "shape above, where we have assumed that batch size and num heads are both 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08ebe71a",
      "metadata": {
        "id": "08ebe71a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c220856b-94bf-4dfd-f3ab-e460d3cac43f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe resulting shape is (B, nh, T, T) or (batch size, num heads, seq len, seq len)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "# TODO: Perform this computation by hand\n",
        "\"\"\"\n",
        "The resulting shape is (B, nh, T, T) or (batch size, num heads, seq len, seq len)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcac08d9",
      "metadata": {
        "id": "bcac08d9"
      },
      "source": [
        "**Task**: What is the value of math.sqrt(k.size(-1))?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "539d89a5",
      "metadata": {
        "id": "539d89a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "5c0d5aee-dd4f-4c4c-a708-7e550e3a54a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nsqrt(hs) == sqrt(emb per head)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "# TODO: Perform this computation by hand\n",
        "\"\"\"\n",
        "sqrt(hs) == sqrt(emb per head)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f25e4cc7",
      "metadata": {
        "id": "f25e4cc7"
      },
      "source": [
        "**Task**: Argue that the line `att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))`  is computing a \"distance\" or \"similarity\" metric between the query at each token position and the key at each token position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb651227",
      "metadata": {
        "id": "fb651227",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "eadc4381-82a8-4f1f-ed12-8bce1d9611d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe first component calculates how q and k are \"aligned\", the number will be large\\nwhen they are \"aligned\". The scaling helps to stablize the value in the previous\\npart as it will go exponentiallt large when the dimension is too big.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "# TODO: Your explanation goes here\n",
        "\"\"\"\n",
        "The first component calculates how q and k are \"aligned\", the number will be large\n",
        "when they are \"aligned\". The scaling helps to stablize the value in the previous\n",
        "part as it will go exponentiallt large when the dimension is too big.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03e5f00d",
      "metadata": {
        "id": "03e5f00d"
      },
      "source": [
        "The following line of code references a `self.bias` parameter, which is\n",
        "defined in the last line of the `__init__()` method. Since `block_size` is\n",
        "quite large, we can understand what `self.bias` looks like by running a\n",
        "similar piece of code below with a smaller `block_size` value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8846b4f",
      "metadata": {
        "id": "d8846b4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6b66dac-8f11-4310-d481-ca370d449cbd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[1., 0., 0., 0., 0.],\n",
              "          [1., 1., 0., 0., 0.],\n",
              "          [1., 1., 1., 0., 0.],\n",
              "          [1., 1., 1., 1., 0.],\n",
              "          [1., 1., 1., 1., 1.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "bias = torch.tril(torch.ones(5, 5)).view(1, 1, 5, 5)\n",
        "bias"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67f7b199",
      "metadata": {
        "id": "67f7b199"
      },
      "source": [
        "**Task**: Explain what the above code returns. Explain how PyTorch\n",
        "broadcasting may be useful for computations involving this tensor---i.e.,\n",
        "why is it okay that the first two dimensions of this tensor are 1,\n",
        "thus assuming that batch size = 1 and num heads = 1?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f37e90c",
      "metadata": {
        "id": "1f37e90c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "ba14107e-42ae-4a88-b313-8a5d4493f7b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe above returns the lower triangle part of a matrix and the cast it to shape (1, 1, 5, 5)\\nPytorch will auto cast the first 2 dimension (in this case) to match the dimension\\nof the other matrix.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "# TODO: Your explanation goes here\n",
        "\"\"\"\n",
        "The above returns the lower triangle part of a matrix and the cast it to shape (1, 1, 5, 5)\n",
        "Pytorch will auto cast the first 2 dimension (in this case) to match the dimension\n",
        "of the other matrix.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "febc960d",
      "metadata": {
        "id": "febc960d"
      },
      "source": [
        "**Task**: We will use a similar technique of running a modified version\n",
        "of the next two lines of code in the `forward()` method to better understand\n",
        "what it does.  Run the below code, and explain what the `masked_fill`\n",
        "function does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccb50280",
      "metadata": {
        "id": "ccb50280",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c320bc3-036d-48bf-9d4e-a9b3cb5c8f86"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.6805, 0.6921, 0.0253, 0.8141, 0.5118],\n",
              "          [0.6076, 0.2428, 0.0739, 0.8267, 0.8288],\n",
              "          [0.8135, 0.5928, 0.8970, 0.4564, 0.9464],\n",
              "          [0.2719, 0.8116, 0.1246, 0.5268, 0.8526],\n",
              "          [0.1134, 0.8380, 0.7821, 0.6496, 0.2038]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "attn = torch.rand(1, 1, 5, 5)\n",
        "attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9f61812",
      "metadata": {
        "id": "d9f61812",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e2fe101-02ce-4655-91ee-a894aa78585a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.6805,   -inf,   -inf,   -inf,   -inf],\n",
              "          [0.6076, 0.2428,   -inf,   -inf,   -inf],\n",
              "          [0.8135, 0.5928, 0.8970,   -inf,   -inf],\n",
              "          [0.2719, 0.8116, 0.1246, 0.5268,   -inf],\n",
              "          [0.1134, 0.8380, 0.7821, 0.6496, 0.2038]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "masked = attn.masked_fill(bias[:,:,:5, :5] == 0, float('-inf'))\n",
        "masked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f557bcdf",
      "metadata": {
        "id": "f557bcdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a16fad1-f214-45f5-9abc-906c47e31026"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.5902, 0.4098, 0.0000, 0.0000, 0.0000],\n",
              "          [0.3461, 0.2776, 0.3763, 0.0000, 0.0000],\n",
              "          [0.2054, 0.3523, 0.1773, 0.2650, 0.0000],\n",
              "          [0.1279, 0.2639, 0.2496, 0.2186, 0.1400]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "out = F.softmax(masked, dim=-1)\n",
        "out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c495c581",
      "metadata": {
        "id": "c495c581",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "5b49ff77-17e6-4d61-f79e-75bb43f4d8d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nmask_fill create a new matrix by replacing the entries that match this condition\\n(bias[:,:,:5, :5] == 0) in attn to the value given.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "# Your explanation goes here\n",
        "\"\"\"\n",
        "mask_fill create a new matrix by replacing the entries that match this condition\n",
        "(bias[:,:,:5, :5] == 0) in attn to the value given.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d430ae25",
      "metadata": {
        "id": "d430ae25"
      },
      "source": [
        "**Graded Task**: This masking is in place so that query tokens cannot\n",
        "\"look up\" key/values that are at a position with a larger index.\n",
        "Explain why this limitation means our GPT model cannot use information\n",
        "in subsequent/later tokens to form an understand of what what is in\n",
        "the current token.\n",
        "(Note: this masking is the \"Causal\" part of Causal Self-Attention!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2346ea89",
      "metadata": {
        "id": "2346ea89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "2ececb9d-d33a-49b9-db49-00abf2ccedf3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nLooking at the mask produced, since it is lower-triangular, only entries (i, j)\\nwhere j <= i has a meaningful value. This is exactly why we cannot \"look up\" the\\npositions with a larger index!\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "# Your explanation goes here\n",
        "\"\"\"\n",
        "Looking at the mask produced, since it is lower-triangular, only entries (i, j)\n",
        "where j <= i has a meaningful value. This is exactly why we cannot \"look up\" the\n",
        "positions with a larger index!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f4a2d0e",
      "metadata": {
        "id": "3f4a2d0e"
      },
      "source": [
        "**Task**: Your answer above explains which positions in the\n",
        "`out` tensor need to be set to zero. Explain why setting the\n",
        "corresponding value of pre-softmax tensor `masked` to `-inf`\n",
        "is necessary. Why can't we set the value of `masked` to 0\n",
        "in these positions?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3310b282",
      "metadata": {
        "id": "3310b282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "5ba13850-0a76-4b07-80d5-0010a07774d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nSince we are using a softmax. exp(0) = 1, meaning that a 0 entry will affect\\nthe probability while exp(-inf) = 0\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "# Your argument goes here\n",
        "\"\"\"\n",
        "Since we are using a softmax. exp(0) = 1, meaning that a 0 entry will affect\n",
        "the probability while exp(-inf) = 0\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48cdda81",
      "metadata": {
        "id": "48cdda81"
      },
      "source": [
        "**Task**: Argue that `out[0,0,0,0]` must always be 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03ebaa49",
      "metadata": {
        "id": "03ebaa49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "451959a7-fcf9-4b0f-b995-946f3f1c8b88"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThere is only one entry, it is the only possible outcome\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "# Your argument goes here\n",
        "\"\"\"\n",
        "There is only one entry, it is the only possible outcome\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47332408",
      "metadata": {
        "id": "47332408"
      },
      "source": [
        "**Task**: Now, `out` in our example is akin to the final value of `att`\n",
        "in the `CausalSelfAttention.forward()` method. Explain why\n",
        "the operation `y = att @ v` computes a weighted sum of values at each\n",
        "token position, where the weights are defined by `att`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fac81d71",
      "metadata": {
        "id": "fac81d71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "abb1048d-50e1-4c92-b5ce-d52f285f952f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nBecause we are using attention mech. We want to know how much should we focus on\\nfor each previous token.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "# Your explanation goes here\n",
        "\"\"\"\n",
        "Because we are using attention mech. We want to know how much should we focus on\n",
        "for each previous token.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da7d85a9",
      "metadata": {
        "id": "da7d85a9"
      },
      "source": [
        "**Task**: The above explanation pertain to a single attention head.\n",
        "Explain why using multiple attention heads allows a token position to\n",
        "consider information from various other positions. Alternatively,\n",
        "explain why using multiple heads might help the network learn different\n",
        "*ways* in which the meaning at one token could depend on other tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da8e485c",
      "metadata": {
        "id": "da8e485c"
      },
      "outputs": [],
      "source": [
        "# Your explanation goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d949e61d",
      "metadata": {
        "id": "d949e61d"
      },
      "source": [
        "**Graded Task**: Compute the total number of parameters in a GPT2 model\n",
        "by computing the following. Please use actual numbers in each case, assuming\n",
        "the GPT2 configuration from above.\n",
        "\n",
        "1. The number of parameters in a `CausalSelfAttention` model.\n",
        "2. The number of parameters in a `MLP` module.\n",
        "3. The number of parameters in a `Block` module.\n",
        "4. The number of parameters in all `Block` modules in a GPT2 model.\n",
        "5. The number of parameters in the `wte` embedding layer in a GPT2 model.\n",
        "6. The total number of parameters in a GPT2 model.\n",
        "\n",
        "Please perform the computation either by hand (and show your work),\n",
        "or with a function that clearly shows the computations.\n",
        "\n",
        "You should see that approximately 30% of the GPT2 weight comes from\n",
        "the `wte` embedding layer. This is why weight tying is used in\n",
        "the `GPT` module!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed1c9b67",
      "metadata": {
        "id": "ed1c9b67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e571fb63-62cf-400a-dbcd-bd33fe08afc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters of CausalSelfAttention is 2362368\n",
            "Parameters of MLP is 4722432\n",
            "Parameters of Block is 7087872\n",
            "Parameters of Blocks is 85054464\n",
            "Parameters of wte is 38633472\n",
            "Parameters of GPT2 is 123689472\n"
          ]
        }
      ],
      "source": [
        "# TODO: Your work goes here\n",
        "block_size = 1024\n",
        "vocab_size = 50304\n",
        "n_layer = 12\n",
        "n_head = 12\n",
        "n_embd = 768\n",
        "CSA_params = n_embd * n_embd * 3 + n_embd * n_embd + 4 * n_embd\n",
        "MLP_params = 4 * n_embd * n_embd * 2  + 5 * n_embd\n",
        "Block_params = CSA_params + MLP_params + 4 * n_embd\n",
        "Blocks_params = Block_params * n_layer\n",
        "wte_params = vocab_size * n_embd\n",
        "wpe_params = 0 # block_size * n_embd\n",
        "ln_f_params = 2 * n_embd\n",
        "lm_head_params = 0 # n_embd * vocab_size\n",
        "GPT2_params = wte_params + wpe_params + Blocks_params + ln_f_params + lm_head_params\n",
        "print(f'Parameters of CausalSelfAttention is {CSA_params}')\n",
        "print(f'Parameters of MLP is {MLP_params}')\n",
        "print(f'Parameters of Block is {Block_params}')\n",
        "print(f'Parameters of Blocks is {Blocks_params}')\n",
        "print(f'Parameters of wte is {wte_params}')\n",
        "print(f'Parameters of GPT2 is {GPT2_params}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec2c2398",
      "metadata": {
        "id": "ec2c2398"
      },
      "source": [
        "## Part 3. Training\n",
        "\n",
        "We are ready to finetune our GPT2 model on the \"Friends\" data set!\n",
        "There is no graded task in this section since training this model can\n",
        "take some time to achieve reasonable performance.\n",
        "\n",
        "To run this part of the lab, you will need to use a GPU. On Google Colab, you can select\n",
        "a session with a GPU by navigating to the \"Runtime\" menu, selecting\n",
        "\"Change runtime type\", and then selecting the \"T4 GPU\" option.\n",
        "\n",
        "We will set up a `config` object to make it easier to store\n",
        "and use configs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9db962ed",
      "metadata": {
        "id": "9db962ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52499d01-3dd7-442f-d17a-a52904a9ca8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: easydict in /usr/local/lib/python3.10/dist-packages (1.13)\n"
          ]
        }
      ],
      "source": [
        "%pip install easydict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b5305a3",
      "metadata": {
        "id": "9b5305a3"
      },
      "outputs": [],
      "source": [
        "import easydict\n",
        "import math\n",
        "import time\n",
        "\n",
        "finetune_config_dict = {\n",
        "  'gradient_accumulation_steps': 32,\n",
        "  'block_size': 256,\n",
        "  'dropout': 0.2,\n",
        "  'bias': False,\n",
        "  'learning_rate': 3e-5,\n",
        "  'weight_decay': 0.1,\n",
        "  'beta1': 0.9,\n",
        "  'beta2': 0.99,\n",
        "  'grad_clip': 1.0,\n",
        "  'decay_lr': False,\n",
        "  'warmup_iters': 100,\n",
        "  'lr_decay_iters': 5000,\n",
        "  'min_lr': 0.0001}\n",
        "config = easydict.EasyDict(finetune_config_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0d6d856",
      "metadata": {
        "id": "e0d6d856"
      },
      "source": [
        "First, we need to load the GPT2 weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd943f5f",
      "metadata": {
        "id": "dd943f5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775,
          "referenced_widgets": [
            "b01af029ce884420ba7ab4f67f93f19e",
            "7478ddbf3c7643ceae27b0639581e152",
            "db9de8f154fe4a2987b147b55c737e98",
            "0c207ae54e0b45eabdc8522f213d6a19",
            "cf1e341aaf8e478697c0772f547710af",
            "88a436a91a7b4539be4ae9e3f50ebae9",
            "091a57840a6046f9a22e01638620558e",
            "45a3bd428e0a4e538dc6979f2133c70a",
            "d8b3f490f4554d7b9efad9846bc1b691",
            "77aa4bb294964c198d9401d9f018fe9e",
            "634ff29486e04802b8f0295a91d4e756",
            "d80225f552194d269b29cf9e1167b4b4",
            "4d01fe8985d949b18551d1110ac4c564",
            "11272aaf484d4a379c4718914965acda",
            "b9995aed7a6144ab85ed23317a1bfb37",
            "2425fa6f66fe4018b096781235ad9d2d",
            "6ccce4245e3846338b50686c781f404f",
            "f3624ca72ab54225b4ab033440552561",
            "650b05ab0a424317bcda028c78417cc9",
            "e63a96c2e76a4d8b939fc1e4ec2a2e12",
            "07ccfb08f7154e66bc71301783a66cb3",
            "272d2a1c30f443468692470a3b934695",
            "2a015f1fae3d4ebcac4cde20ca480a5a",
            "25be975ed5ca4d2996bf506b02ac0606",
            "b903adf58147470096d0e0eb17c1b592",
            "5eee91ee07c9480dad9c2708fb2f0a97",
            "be40545c0ae2440380e966c260b22c10",
            "a08a0e80af104258a3e07c619a27dd58",
            "6f4d0006aae84942bfbca8576329c63d",
            "428ca9bbd55c4908b4936472c9228d99",
            "dba6fe95bbe1495cb09a1374a3efb1ab",
            "b052dfecbc55414092b777837a4653fd",
            "6ce5daeac2644aab86b602592bbd8804"
          ]
        },
        "outputId": "c28c6940-bcdb-46b7-bc7e-66bccf346a3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.2\n",
            "number of parameters: 123.65M\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b01af029ce884420ba7ab4f67f93f19e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d80225f552194d269b29cf9e1167b4b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a015f1fae3d4ebcac4cde20ca480a5a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.2, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "# initialize from OpenAI GPT-2 weights\n",
        "override_args = dict(dropout=config.dropout)\n",
        "model = GPT.from_pretrained('gpt2', override_args)\n",
        "\n",
        "# crop down the model block size using model surgery\n",
        "if config.block_size < model.config.block_size:\n",
        "    model.crop_block_size(config.block_size)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available()  else 'cpu'\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "014bf5a9",
      "metadata": {
        "id": "014bf5a9"
      },
      "source": [
        "**Task**: Explain why reducing the `block_size` do not significantly\n",
        "reduce the number of parameters, but *does* significantly reduce\n",
        "memory usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20e39aa8",
      "metadata": {
        "id": "20e39aa8"
      },
      "outputs": [],
      "source": [
        "# TODO: Include your explanation here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "678c0557",
      "metadata": {
        "id": "678c0557"
      },
      "source": [
        "There are some additional helpers to improve training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2f126cb",
      "metadata": {
        "id": "b2f126cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8385b7ae-7639-4dbd-c8b1-8fc0f867eee0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from contextlib import nullcontext\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device == 'cpu' else torch.amp.autocast(device_type=device, dtype=ptdtype)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(config, it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < config.warmup_iters:\n",
        "        return config.learning_rate * it / config.warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > config.lr_decay_iters:\n",
        "        return config.min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - config.warmup_iters) / (config.lr_decay_iters - config.warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return config.min_lr + coeff * (config.learning_rate - config.min_lr)\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, train_dataset, val_dataset, block_size):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        dataset = train_dataset if split == 'train' else val_dataset\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(dataset, block_size, batch_size, device)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62dbfec1",
      "metadata": {
        "id": "62dbfec1"
      },
      "source": [
        "Now we can begin the training loop. You may need to increase `max_iter`\n",
        "to obtain good results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4014bf8e",
      "metadata": {
        "id": "4014bf8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0446d03-4a1f-4681-d28c-b9d19418bbc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 50, with 123,728,640 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 3.3885, val loss 3.3026\n",
            "iter 0: loss 4.2193, time 7200.32ms, mfu -100.00%\n",
            "step 10: train loss 3.1913, val loss 3.2362\n",
            "iter 10: loss 3.6436, time 2264.23ms, mfu 0.89%\n",
            "step 20: train loss 3.1206, val loss 3.1943\n",
            "iter 20: loss 3.4520, time 2037.57ms, mfu 0.90%\n",
            "step 30: train loss 3.0929, val loss 3.2047\n",
            "iter 30: loss 3.3986, time 2040.25ms, mfu 0.91%\n",
            "step 40: train loss 3.1103, val loss 3.1811\n",
            "iter 40: loss 3.3459, time 2052.55ms, mfu 0.92%\n",
            "step 50: train loss 3.0319, val loss 3.1109\n",
            "iter 50: loss 2.7773, time 2073.79ms, mfu 0.92%\n",
            "step 60: train loss 2.9840, val loss 3.1450\n",
            "iter 60: loss 3.4185, time 2080.34ms, mfu 0.93%\n",
            "step 70: train loss 2.9881, val loss 3.1572\n",
            "iter 70: loss 3.1751, time 3044.51ms, mfu 0.90%\n",
            "step 80: train loss 2.8914, val loss 3.1805\n",
            "iter 80: loss 3.8361, time 2081.65ms, mfu 0.91%\n",
            "step 90: train loss 2.9832, val loss 3.1893\n",
            "iter 90: loss 3.4600, time 2083.27ms, mfu 0.92%\n",
            "step 100: train loss 2.9446, val loss 3.1602\n",
            "iter 100: loss 3.1786, time 2106.63ms, mfu 0.92%\n",
            "step 110: train loss 2.9800, val loss 3.1419\n",
            "iter 110: loss 3.0156, time 2080.97ms, mfu 0.93%\n",
            "step 120: train loss 2.9024, val loss 3.1535\n",
            "iter 120: loss 3.1064, time 2080.57ms, mfu 0.93%\n",
            "step 130: train loss 2.8129, val loss 3.1878\n",
            "iter 130: loss 3.0438, time 2079.85ms, mfu 0.93%\n"
          ]
        }
      ],
      "source": [
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "eval_interval = 10\n",
        "log_interval = 10\n",
        "eval_iters = 40\n",
        "max_iters = 500\n",
        "batch_size = 1\n",
        "\n",
        "# optimizer\n",
        "optimizer = model.configure_optimizers(config.weight_decay, config.learning_rate,\n",
        "   (config.beta1, config.beta2), device)\n",
        "\n",
        "# training loop\n",
        "X, Y = get_batch(train_data, config.block_size, batch_size, device) # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "raw_model = model # unwrap DDP container if needed\n",
        "running_mfu = -1.0\n",
        "while True:\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(config, iter_num) if config.decay_lr else config.learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0:\n",
        "        losses = estimate_loss(model, train_data, val_data, config.block_size)\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(config.gradient_accumulation_steps):\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / config.gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch(train_data, config.block_size, batch_size, device)\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    # clip the gradient\n",
        "    if config.grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
        "\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0:\n",
        "        # get loss as float. note: this is a CPU-GPU sync point\n",
        "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "        lossf = loss.item() * config.gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(batch_size * config.gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f72d0044",
      "metadata": {
        "id": "f72d0044"
      },
      "source": [
        "Here is some code you can use to generate a sequence using the fine-tuned\n",
        "GPT2 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15a09622",
      "metadata": {
        "id": "15a09622"
      },
      "outputs": [],
      "source": [
        "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
        "out_dir = 'out' # ignored if init_from is not 'resume'\n",
        "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
        "num_samples = 10 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
        "decode = lambda l: enc.decode(l)\n",
        "\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# model = finetuned_model\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b01af029ce884420ba7ab4f67f93f19e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7478ddbf3c7643ceae27b0639581e152",
              "IPY_MODEL_db9de8f154fe4a2987b147b55c737e98",
              "IPY_MODEL_0c207ae54e0b45eabdc8522f213d6a19"
            ],
            "layout": "IPY_MODEL_cf1e341aaf8e478697c0772f547710af"
          }
        },
        "7478ddbf3c7643ceae27b0639581e152": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88a436a91a7b4539be4ae9e3f50ebae9",
            "placeholder": "​",
            "style": "IPY_MODEL_091a57840a6046f9a22e01638620558e",
            "value": "config.json: 100%"
          }
        },
        "db9de8f154fe4a2987b147b55c737e98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45a3bd428e0a4e538dc6979f2133c70a",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8b3f490f4554d7b9efad9846bc1b691",
            "value": 665
          }
        },
        "0c207ae54e0b45eabdc8522f213d6a19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77aa4bb294964c198d9401d9f018fe9e",
            "placeholder": "​",
            "style": "IPY_MODEL_634ff29486e04802b8f0295a91d4e756",
            "value": " 665/665 [00:00&lt;00:00, 21.1kB/s]"
          }
        },
        "cf1e341aaf8e478697c0772f547710af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88a436a91a7b4539be4ae9e3f50ebae9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "091a57840a6046f9a22e01638620558e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45a3bd428e0a4e538dc6979f2133c70a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8b3f490f4554d7b9efad9846bc1b691": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "77aa4bb294964c198d9401d9f018fe9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "634ff29486e04802b8f0295a91d4e756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d80225f552194d269b29cf9e1167b4b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d01fe8985d949b18551d1110ac4c564",
              "IPY_MODEL_11272aaf484d4a379c4718914965acda",
              "IPY_MODEL_b9995aed7a6144ab85ed23317a1bfb37"
            ],
            "layout": "IPY_MODEL_2425fa6f66fe4018b096781235ad9d2d"
          }
        },
        "4d01fe8985d949b18551d1110ac4c564": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ccce4245e3846338b50686c781f404f",
            "placeholder": "​",
            "style": "IPY_MODEL_f3624ca72ab54225b4ab033440552561",
            "value": "model.safetensors: 100%"
          }
        },
        "11272aaf484d4a379c4718914965acda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_650b05ab0a424317bcda028c78417cc9",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e63a96c2e76a4d8b939fc1e4ec2a2e12",
            "value": 548105171
          }
        },
        "b9995aed7a6144ab85ed23317a1bfb37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07ccfb08f7154e66bc71301783a66cb3",
            "placeholder": "​",
            "style": "IPY_MODEL_272d2a1c30f443468692470a3b934695",
            "value": " 548M/548M [00:06&lt;00:00, 109MB/s]"
          }
        },
        "2425fa6f66fe4018b096781235ad9d2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ccce4245e3846338b50686c781f404f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3624ca72ab54225b4ab033440552561": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "650b05ab0a424317bcda028c78417cc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e63a96c2e76a4d8b939fc1e4ec2a2e12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "07ccfb08f7154e66bc71301783a66cb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "272d2a1c30f443468692470a3b934695": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a015f1fae3d4ebcac4cde20ca480a5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_25be975ed5ca4d2996bf506b02ac0606",
              "IPY_MODEL_b903adf58147470096d0e0eb17c1b592",
              "IPY_MODEL_5eee91ee07c9480dad9c2708fb2f0a97"
            ],
            "layout": "IPY_MODEL_be40545c0ae2440380e966c260b22c10"
          }
        },
        "25be975ed5ca4d2996bf506b02ac0606": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a08a0e80af104258a3e07c619a27dd58",
            "placeholder": "​",
            "style": "IPY_MODEL_6f4d0006aae84942bfbca8576329c63d",
            "value": "generation_config.json: 100%"
          }
        },
        "b903adf58147470096d0e0eb17c1b592": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_428ca9bbd55c4908b4936472c9228d99",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dba6fe95bbe1495cb09a1374a3efb1ab",
            "value": 124
          }
        },
        "5eee91ee07c9480dad9c2708fb2f0a97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b052dfecbc55414092b777837a4653fd",
            "placeholder": "​",
            "style": "IPY_MODEL_6ce5daeac2644aab86b602592bbd8804",
            "value": " 124/124 [00:00&lt;00:00, 1.59kB/s]"
          }
        },
        "be40545c0ae2440380e966c260b22c10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a08a0e80af104258a3e07c619a27dd58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f4d0006aae84942bfbca8576329c63d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "428ca9bbd55c4908b4936472c9228d99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dba6fe95bbe1495cb09a1374a3efb1ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b052dfecbc55414092b777837a4653fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ce5daeac2644aab86b602592bbd8804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}