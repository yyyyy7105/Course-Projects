{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "561e4aa0",
      "metadata": {
        "id": "561e4aa0"
      },
      "source": [
        "# CSC413 Lab 1: Linear Models\n",
        "\n",
        "In this lab, we will review **linear models** for classification, which was discussed in depth in CSC311.\n",
        "We will use this as an opportunity to review key ideas, including the splitting of the dataset into\n",
        "train/validation/test sets, optimization methods using Stochastic Gradient Descent, and so on.\n",
        "This lab reviews Python libraries that you have used in CSC311, including `numpy`, `matplotlib` and others.\n",
        "\n",
        "The main aim of this lab is to revisit the PyTorch library. PyTorch provides\n",
        "automatic differentiation capabilities and other neural network tools.\n",
        "This means that **we do not need to compute gradients ourselves**! Instead,\n",
        "we rely on PyTorch to build the computation graph and compute gradients.\n",
        "PyTorch can do this because it knows how to compute gradients for simple\n",
        "operations like addition, multiplication, ReLU activation, and common\n",
        "functions like exponentials, logarithms, and so on. The neural networks\n",
        "we build require computation that are combinations of these simple operations.\n",
        "\n",
        "For now, we will we solve a multi-class classification problem with PyTorch.\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "\n",
        "1. Implement and train a multi-class logistic regression model using PyTorch.\n",
        "2. Compute the accuracy metric for a machine learning model.\n",
        "3. Compute numerically, via numpy, the gradient of a linear model.\n",
        "4. Check that PyTorch correctly computes gradients for a linear model via automatic differentiation.\n",
        "5. Identify and explain the elements of the training loop in a PyTorch implementation.\n",
        "6. Identify optimization parameters and hyperparameters, and explain how hyperparameter choices impact training, underfitting and overfitting (i.e. bias/variance decomposition).\n",
        "\n",
        "Please work in groups of 1-2 during the lab, but submit your own solution individually.\n",
        "\n",
        "## Submission\n",
        "\n",
        "If you are working with a partner, start by creating a group on Markus. If you are working alone,\n",
        "click \"Working Alone\".\n",
        "\n",
        "Submit the ipynb file `lab01.ipynb` on Markus\n",
        "**containing all your solutions to the Graded Task**s.\n",
        "Your notebook file must contain your code **and outputs** where applicable,\n",
        "including printed lines and images.\n",
        "Your TA will not run your code for the purpose of grading.\n",
        "\n",
        "For this lab, you should submit the following:\n",
        "\n",
        "- Part 1. Your explanation of the purpose of the training and validation sets. (1 point)\n",
        "- Part 2. Your implementation of `accuracy_basic`. (2 point)\n",
        "- Part 2. Your implementation of `accuracy_vectorized`. (1 point)\n",
        "- Part 2. Your implementation of `accuracy`. (1 point)\n",
        "- Part 3. Your computation of `model_bias_grad`. (2 points)\n",
        "- Part 4. Your completion of `train_model`. (1 point)\n",
        "- Part 5. Your list of hyperparameters. (1 point)\n",
        "- Part 5. Your explanation of what happens if the learning rate is too large. (1 point)\n",
        "\n",
        "\n",
        "## Google Colab Setup\n",
        "\n",
        "We will use Google Colab to open the provided IPython Notebook (.ipynb) file.\n",
        "This tool allows us to write and execute Python code through our browser, without any environmental setup.\n",
        "\n",
        "Here are the steps to open .ipynb files on Google Colab.\n",
        "\n",
        "1. Download `lab01.ipynb`, available from the Quercus course website.\n",
        "2. Click on the following link to open Google Colab: https://colab.research.google.com/\n",
        "3. Click \"Upload\", then choose the file which has been downloaded in step 1.\n",
        "\n",
        "And that's it! Now we can start writing the codes, creating the new code or text cell, etc.\n",
        "\n",
        "Here are some basic functionalities and features that you might find useful.\n",
        "\n",
        "1. Running a cell \\\n",
        "    Click the run button on the left side of the code cell (looks like a “play” button with a triangle in a circle) \\\n",
        "    or \\\n",
        "    press SHIFT + ENTER.\n",
        "\n",
        "2. Installing libraries using Bash Commands \\\n",
        "    Although most of the commonly used libraries (e.g. NumPy, Pandas, Matplotlib) are pre-installed,\n",
        "    we may occasionally ask you to install new libraries or run other bash commands.\n",
        "    Bash commands can be run by prefixing instructions in a code cell with '!' in Google Colab (One exception: 'cd' command can be run by prefixing with '%'), e.g. `!pip install [package name]`\n",
        "\n",
        "3. Mounting Google Drive \\\n",
        "    You may optionally mount Google Drive.\n",
        "    Click the files button on the left pane, then click on 'mount drive' button (looks like a file icon with a google drive logo). \\\n",
        "    or \\\n",
        "    Run the following code snippet:\n",
        "    ```\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    ```\n",
        "    By mounting the drive, we can use any files or folders in our drive by using the path as follows:\n",
        "    ```\n",
        "    /content/drive/MyDrive/[folder name]\n",
        "    ```\n",
        "    For example, we can read the csv file uploaded in the drive using Pandas library as follows:\n",
        "    ```\n",
        "    pd.read_csv('/content/drive/MyDrive/myfolder/myfile.csv')\n",
        "    ```\n",
        "\n",
        "Now, we are ready to import the necessary packages and begin our lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2997ed62",
      "metadata": {
        "id": "2997ed62"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6ecb330",
      "metadata": {
        "id": "f6ecb330"
      },
      "source": [
        "## Part 1. Data\n",
        "\n",
        "We will use the MNIST data set, which consists of hand-written digits. This dataset is available within\n",
        "the `torchvision.datasets` library. The dataset creators divided the MNIST imgages into a training and\n",
        "test set, so that different researchers report test accuracy on a consistent set of images.\n",
        "(Recall that the test set is to be set aside and **not** used during training or to make any model decisions,\n",
        "and that it is used to estimate *how well your models generalize* to new data that it has never seen before.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "112f0ac6",
      "metadata": {
        "id": "112f0ac6",
        "outputId": "81446558-27d7-4823-fab0-da1eff5f678c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 35702475.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 73238025.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 34383813.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5192294.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from torchvision.datasets import MNIST\n",
        "\n",
        "mnist_train = MNIST(root=\".\",      # where on the disk to store the data\n",
        "                    download=True, # download the data if it does not already exist\n",
        "                    train=True)    # use the training set (rather than the test set)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa2f989b",
      "metadata": {
        "id": "aa2f989b"
      },
      "source": [
        "**Task:** If different practitioners are exploring machine learning models for the same task and data set,\n",
        "why is it important that practitioners report their test performance (e.g., accuracy)\n",
        "on the same test set?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a50df611",
      "metadata": {
        "id": "a50df611"
      },
      "outputs": [],
      "source": [
        "# TODO: Write your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d18e4ec1",
      "metadata": {
        "id": "d18e4ec1"
      },
      "source": [
        "It is always a good idea to visually inspect our data before working with it.\n",
        "First, let's take a look at the first element of the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c9d82b4",
      "metadata": {
        "id": "1c9d82b4"
      },
      "outputs": [],
      "source": [
        "print(mnist_train[0]) # a tuple consisting of the image, and the label (5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "362e5965",
      "metadata": {
        "id": "362e5965"
      },
      "source": [
        "The image can be displayed on Google Colab using matplotlib:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f11d9208",
      "metadata": {
        "id": "f11d9208"
      },
      "outputs": [],
      "source": [
        "plt.imshow(mnist_train[0][0], cmap='gray') # display the image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55f679d5",
      "metadata": {
        "id": "55f679d5"
      },
      "source": [
        "It is important to note that images are represented using numbers on your machine.\n",
        "Converting this image into a numpy array shows a representation of the image using\n",
        "28x28 numbers, each representing a pixel value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83107664",
      "metadata": {
        "id": "83107664"
      },
      "outputs": [],
      "source": [
        "np.array(mnist_train[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afbe4d5f",
      "metadata": {
        "id": "afbe4d5f"
      },
      "source": [
        "**Task:** What does the numerical value 0 (smallest possible value) mean in the image?\n",
        "What about the largest possible value, 255?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "101c8b05",
      "metadata": {
        "id": "101c8b05"
      },
      "outputs": [],
      "source": [
        "# TODO: Write your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32b8ce96",
      "metadata": {
        "id": "32b8ce96"
      },
      "source": [
        "For our purposes, we will only use the first 5000 elements of the training set.\n",
        "This is to make training faster.\n",
        "\n",
        "PyTorch also makes it easy to apply pre-processing transformations to the data,\n",
        "for example to normalize the data prior to using for training.\n",
        "We will use the standard preprocessing functions to *transform the images into tensors* for PyTorch to be able to use.\n",
        "This transformation also changes the values to be floating-point numbers between 0 and 1.\n",
        "Performing this transformation to PyTorch tensors now makes it easier to use PyTorch functionalities to\n",
        "help us create minibatches with this data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "257f5880",
      "metadata": {
        "id": "257f5880"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "mnist_data = MNIST(root=\".\",      # where on the disk to store the data\n",
        "                   download=True, # download the data if it does not already exist\n",
        "                   train=True,    # use the canonical training set (rather than the test set)\n",
        "                   transform=transforms.ToTensor()) # transforms the images into PyTorch tensors\n",
        "mnist_data = list(mnist_data)[:5000]\n",
        "\n",
        "print(mnist_data[0]) # a tuple consisting of a PyTorch tensor of shape (1, 28, 28) and an integer target label"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0f0519f",
      "metadata": {
        "id": "e0f0519f"
      },
      "source": [
        "We will split the data set into 3000 for training, 1000 for validation, and 1000 for test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2b676b3",
      "metadata": {
        "id": "e2b676b3"
      },
      "outputs": [],
      "source": [
        "train_data = mnist_data[:3000]\n",
        "val_data   = mnist_data[3000:4000]\n",
        "test_data  = mnist_data[4000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85088fb7",
      "metadata": {
        "id": "85088fb7"
      },
      "source": [
        "**Graded Task**: We described, above, that the purpose of the **test set** is to estimate how well\n",
        "our models would generalize to new data that it has never seen before.\n",
        "What are the purposes of the training and validation sets?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8daea29",
      "metadata": {
        "id": "f8daea29"
      },
      "outputs": [],
      "source": [
        "# TODO: Write your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c544a833",
      "metadata": {
        "id": "c544a833"
      },
      "source": [
        "## Part 2. A Linear Model in PyTorch\n",
        "\n",
        "You may recall from CSC311 that in machine learning, we often describe a model by first describing\n",
        "how to make predictions with a model (e.g., multi-class classification), and **then** describe\n",
        "how to find appropriate weights (e.g., an optimization method like gradient descent).\n",
        "We will follow that process here.\n",
        "\n",
        "Recall that, mathematically, the multi-class classification model can be written as follows:\n",
        "\n",
        "$${\\bf y} = \\textrm{softmax}({\\bf W} {\\bf x} + {\\bf b})$$\n",
        "\n",
        "Where the ${\\bf x}$ vector represents the input (i.e., the vector representation of the MNIST image),\n",
        "and the ${\\bf y}$ vector contains the predicted probably of the image being in each class (i.e.,\n",
        "predicted probability of the image being of each digit 0-9).\n",
        "\n",
        "**Task**: In the MNIST image classification task, what are the shapes of the quantities\n",
        "${\\bf x}$, ${\\bf W}$, ${\\bf b}$ and ${\\bf y}$?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "139e72f2",
      "metadata": {
        "id": "139e72f2"
      },
      "outputs": [],
      "source": [
        "# TODO: Write your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8758b01",
      "metadata": {
        "id": "c8758b01"
      },
      "source": [
        "The matrix ${\\bf W}$ and ${\\bf b}$ are the parameters of the model. The matrix  ${\\bf W}$ is\n",
        "sometimes called the *weight matrix* and the vector ${\\bf b}$ the *bias vector*, but these parameters\n",
        "taken together are often referred to collectively as the **weights**.\n",
        "\"Good\" values of the parameters ${\\bf W}$ and ${\\bf b}$ are those that would produce values of\n",
        "the vector ${\\bf y}$ that more accurately match the actual target label. We will need to discuss\n",
        "what \"good\" means and how to measure \"good\"ness and optimize it. For now, let's begin by applying\n",
        "and analyzing a \"bad\" model: a model where the parameters ${\\bf W}$ and ${\\bf b}$ are chosen\n",
        "randomly.\n",
        "\n",
        "\n",
        "Notice that there are two parts to the computation $\\textrm{softmax}({\\bf W} {\\bf x} + {\\bf b})$:\n",
        "there is a *linear transformation* on ${\\bf X}$, and then there is a softmax operation.\n",
        "PyTorch models these two components separately.\n",
        "\n",
        "The **linear transformation** is modeled as a Python class in PyTorch.\n",
        "Since the parameters ${\\bf W}$ and ${\\bf b}$ are parameters of the linear transformation\n",
        "portion of the computation, the weights and biases are class attributes.\n",
        "The output of the linear transformation step is typically denoted using the symbol ${\\bf z}$,\n",
        "and is called the **logit** (or unnormalized logits)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0376b376",
      "metadata": {
        "id": "0376b376"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "model = nn.Linear(in_features=784,\n",
        "                  out_features=10)\n",
        "\n",
        "print(model.weight) # the weight parameter, initialized to random values\n",
        "print(model.bias)   # the bias parameter, initialized to random values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41ee800b",
      "metadata": {
        "id": "41ee800b"
      },
      "source": [
        "This `model` object can be called like a function to perform the computation ${\\bf W}{\\bf x} + {\\bf b}$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee8d2c84",
      "metadata": {
        "id": "ee8d2c84"
      },
      "outputs": [],
      "source": [
        "# reshape the input image into the shape [1, 784]\n",
        "# PyTorch always expects inputs of shape [batch_size, num_features]\n",
        "x = train_data[0][0].reshape(1, 784)\n",
        "\n",
        "# Computes z = Wx + b, also called the *logit*\n",
        "z = model(x)\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73894f67",
      "metadata": {
        "id": "73894f67"
      },
      "source": [
        "The **softmax operation** has no trainable parameters (i.e., no numbers that we tune that would effect\n",
        "the predictions of our model).\n",
        "Torch has a function `torch.softmax` that performs this operation, which\n",
        "normalizes the prediction so that the prediction represents a probability distribution.\n",
        "The `dim` parameter to the function tells PyTorch which dimension represent the different label classes\n",
        "(as opposed to, say, the dimension that represents different images in a batch)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c118a748",
      "metadata": {
        "id": "c118a748"
      },
      "outputs": [],
      "source": [
        "y = torch.softmax(z, dim=1)\n",
        "print(y) # notice that this represents a probability distribution!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98e71db6",
      "metadata": {
        "id": "98e71db6"
      },
      "source": [
        "If we are looking for a discrete/point prediction rather than a probability distribution,\n",
        "we will typically choose the label that the model believes to be **most probable**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43bb6b1a",
      "metadata": {
        "id": "43bb6b1a"
      },
      "outputs": [],
      "source": [
        "pred = torch.argmax(y, axis=1)\n",
        "print(pred) # a prediction of which class/digit the image belong to"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "218c0266",
      "metadata": {
        "id": "218c0266"
      },
      "source": [
        "Note that if all we wanted was a discrete prediction, we need not compute the softmax!\n",
        "(Why is that? How can we prove this property mathematically, using the definition of\n",
        "the softmax operation?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c487df46",
      "metadata": {
        "id": "c487df46"
      },
      "outputs": [],
      "source": [
        "pred = torch.argmax(z, axis=1)\n",
        "print(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e141e496",
      "metadata": {
        "id": "e141e496"
      },
      "source": [
        "**Graded Task**: Complete the function below, which computes the **accuracy** of a PyTorch model\n",
        "over a dataset. The accuracy metric is the proportion of predictions made that is correct,\n",
        "or:\n",
        "\n",
        "$$\\frac{\\textrm{the number of correct predictions}}{\\textrm{total number of predictions made}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89778660",
      "metadata": {
        "id": "89778660"
      },
      "outputs": [],
      "source": [
        "def accuracy_basic(model, dataset):\n",
        "    \"\"\"\n",
        "    Compute the accuracy of `model` over the `dataset`.\n",
        "    We will take the **most probable class**\n",
        "    as the class predicted by the model.\n",
        "\n",
        "    Parameters:\n",
        "        `model` - A torch.nn model. We will only be passing `nn.Linear` models.\n",
        "                  However, to make your code more generally useful, do not access\n",
        "                  `model.weight` and `model.bias` parameters directly. These\n",
        "                  class attributes may not exist for other kinds of models.\n",
        "        `dataset` - A list of 2-tuples of the form (x, t), where `x` is a PyTorch\n",
        "                  tensor of shape [1, 28, 28] representing an MNIST image,\n",
        "                  and `t` is the corresponding target label\n",
        "\n",
        "    Returns: a floating-point value between 0 and 1.\n",
        "    \"\"\"\n",
        "    total = 0      # count the total number of predictions made\n",
        "    correct = 0    # count the number of correct predictions made\n",
        "    for img, t in dataset:\n",
        "        x = None # TODO - what should the input be?\n",
        "        z = None # TODO - how can we compute z = Wx + b using `model`?\n",
        "        pred = None # TODO - how can we obtain a point prediction?\n",
        "        if t == pred:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "print(\"Accuracy over the training set:\")\n",
        "print(accuracy_basic(model, train_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6a6d84e",
      "metadata": {
        "id": "b6a6d84e"
      },
      "source": [
        "**Task**: Explain why we would expect the training accuracy above to be poor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07f8bd3b",
      "metadata": {
        "id": "07f8bd3b"
      },
      "outputs": [],
      "source": [
        "# TODO: Your explanation goes here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67ead752",
      "metadata": {
        "id": "67ead752"
      },
      "source": [
        "One other nice thing about our `nn.Linear` model is that PyTorch **vectorizes**\n",
        "computations for us: we can make predictions for *many* images at the same time.\n",
        "Here is a rudimentary example where we make predictions for the first three\n",
        "images of our training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f86d0304",
      "metadata": {
        "id": "f86d0304"
      },
      "outputs": [],
      "source": [
        "x1 = train_data[0][0].reshape(1, 784)\n",
        "x2 = train_data[1][0].reshape(1, 784)\n",
        "x3 = train_data[2][0].reshape(1, 784)\n",
        "\n",
        "X = torch.cat([x1, x2, x3]).reshape(-1, 784) # note the -1 value here, explained below\n",
        "print(X.shape) # Pytorch figures out that the shape of this tensor needs to be [3, 784]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaceffd6",
      "metadata": {
        "id": "eaceffd6"
      },
      "source": [
        "The above code uses a features of PyTorch's `reshape()` method that allows you to\n",
        "use the size `-1` as a placeholder value and let PyTorch figure out what the correct\n",
        "size should be.\n",
        "\n",
        "Now, we can make predictions for all 3 images simultaneously"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9274378",
      "metadata": {
        "id": "d9274378"
      },
      "outputs": [],
      "source": [
        "z = model(X)\n",
        "y = torch.softmax(z, dim=1)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c273455",
      "metadata": {
        "id": "7c273455"
      },
      "source": [
        "**Task** Complete the function `accuracy_vectorized` that outputs the same\n",
        "result as `accuracy_basic`, but uses vectorization to\n",
        "compute predictions for **all** inputs in the `dataset` simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c0bd53d",
      "metadata": {
        "id": "2c0bd53d"
      },
      "outputs": [],
      "source": [
        "def accuracy_vectorized(model, dataset):\n",
        "    \"\"\"\n",
        "    Same signature as the `accuracy_basic()` function, but the call to model() is vectorized\n",
        "    \"\"\"\n",
        "    X = torch.concat([x for x, t in dataset])\n",
        "    t = torch.Tensor([t for x, t in dataset])\n",
        "    z = None # TODO: use a single call to model() to compute the prediction for all inputs\n",
        "    pred = None # TODO: `pred` should have the same shape as `t`\n",
        "    correct = int(torch.sum(t == pred)) # count the number of correct predictions\n",
        "    total = t.shape[0]                  # count the total number of predictions made\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "289ed2ea",
      "metadata": {
        "id": "289ed2ea"
      },
      "source": [
        "**Task**: Compare the runtime of `accuracy_basic` and `accuracy_vectorized` by running the two cells\n",
        "below. The line `%%time` prints the amount of time that Colab takes to run the code in the cell.\n",
        "The function call is repeated 100 times so that we can more clearly see the difference in\n",
        "runtime. Using what you learned from CSC311, explain why `accuracy_vectorized` is faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01fcebf3",
      "metadata": {
        "id": "01fcebf3"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "for i in range(100):\n",
        "    accuracy_basic(model, train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad8d2acc",
      "metadata": {
        "id": "ad8d2acc"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "for i in range(100):\n",
        "    accuracy_vectorized(model, train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4abeb7ee",
      "metadata": {
        "id": "4abeb7ee"
      },
      "outputs": [],
      "source": [
        "# TODO: Your explanation goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ee40246",
      "metadata": {
        "id": "5ee40246"
      },
      "source": [
        "If our data set is large, feeding all inputs into the model at the same time may result\n",
        "in an out of memory error.\n",
        "Thus, we may find PyTorch's `DataLoader` useful. This class takes our data set and the\n",
        "desired batch size, and splits the data into mini-batches of that size. We can use\n",
        "a loop to iterate over the minibatches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e68219de",
      "metadata": {
        "id": "e68219de"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=100)\n",
        "for X, t in train_loader:\n",
        "    print(X.shape)\n",
        "    print(t.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69a47a6e",
      "metadata": {
        "id": "69a47a6e"
      },
      "source": [
        "**Task**: Complete the definition of the `accuracy` function below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "053e6ef3",
      "metadata": {
        "id": "053e6ef3"
      },
      "outputs": [],
      "source": [
        "def accuracy(model, dataset):\n",
        "    \"\"\"\n",
        "    Same signature as the `accuracy_basic()` function, but we will use a DataLoader and process\n",
        "    100 images at a time\n",
        "    \"\"\"\n",
        "    correct, total = 0, 0\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=100)\n",
        "    for X, t in loader:\n",
        "        z = None # TODO: use a single call to `model()` here as before\n",
        "        pred = None # TODO: `pred` should have the same shape as `t`\n",
        "        # TODO: update `correct` and `total`\n",
        "    return correct / total\n",
        "\n",
        "accuracy(model, train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8da6d245",
      "metadata": {
        "id": "8da6d245"
      },
      "source": [
        "## Part 3. Cross Entropy Loss and Automatic Gradient Computation\n",
        "\n",
        "Now that we understand how a linear model makes predictions, we can explore how to modify its\n",
        "parameters to produce a model that makes \"better\" predictions. To do this, we will define\n",
        "a measure of \"good\"ness (or rather, \"bad\"ness) of a model that is **differentiable with respect\n",
        "to the parameters**. Differentiability is important, because it allows us to compute derivatives\n",
        "with respect to these parameters, which tells us how to tune the parameters to decrease \"bad\"ness.\n",
        "\n",
        "This \"badness\" metric is called a **loss function**. A loss function compares the model prediction\n",
        "against the ground-truth target and produces a value representing how different the prediction is\n",
        "from the target.  Like in CSC311, we will use the **Cross-Entropy Loss** for multi-class classification.\n",
        "In statistics courses you may learn about the theoretical reasons why the cross-entropy loss is\n",
        "appropriate for the multi-class classification task.\n",
        "\n",
        "$$\\mathcal{L}(y, t) = - \\sum_{k=1}^K t_k \\log(y_k) = - \\mathbf{t}^\\top \\log(\\mathbf{y})$$\n",
        "\n",
        "You can read more about PyTorch's implementation of the Cross-entropy loss here:\n",
        "[https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
        "One key thing to note is that the cross entropy loss takes as input the **unnormalized logits** (the z's) and *not*\n",
        "the post-softmax, normalized probabilities (the y's)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0715ca74",
      "metadata": {
        "id": "0715ca74"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a01a963",
      "metadata": {
        "id": "9a01a963"
      },
      "source": [
        "Before we go further, let's get some more intuition about the cross-entropy loss.\n",
        "We first demonstrate the cross entropy loss in action by computing hte \"badness\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32b61e9f",
      "metadata": {
        "id": "32b61e9f"
      },
      "outputs": [],
      "source": [
        "x = train_data[0][0].reshape(1, 784)        # input\n",
        "t = torch.Tensor([train_data[0][1]]).long() # target label, represented as an integer index\n",
        "z = model(x.reshape(-1, 784))               # prediction logit\n",
        "\n",
        "loss = criterion(z, t)\n",
        "\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad29a71f",
      "metadata": {
        "id": "ad29a71f"
      },
      "source": [
        "**Graded Task**: Consider the code below.  What value of `label` would produce the lowest cross-entropy loss?\n",
        "Why? To answer this question, start by exploring the arguments passed to the call to `criterion`, and\n",
        "form an understanding of what those arugments represent. (Why are there 3 possible values of `label`?\n",
        "What do the 3 floating-point values in the first argument to `criterion` represent?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fe30e12",
      "metadata": {
        "id": "3fe30e12"
      },
      "outputs": [],
      "source": [
        "label = None # 0, 1, or 2?\n",
        "\n",
        "loss = criterion(torch.Tensor([[1.5, 2.2, 3.2]]),\n",
        "                 torch.Tensor([label]).long())\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d23bf1a6",
      "metadata": {
        "id": "d23bf1a6"
      },
      "outputs": [],
      "source": [
        "# TODO: Explain why the label choice produces the lowest loss."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8d8a813",
      "metadata": {
        "id": "e8d8a813"
      },
      "source": [
        "Now that we have an understanding of the cross-entropy loss, we can begin to understand how PyTorch computes\n",
        "gradients.\n",
        "Notice that when we print the variable `loss` above,\n",
        "the value printed is not only a numerical value, but also has other information attached.\n",
        "These information help PyTorch compute gradients, and these gradients can be propagated backwards\n",
        "using the `loss.backward()` method.\n",
        "\n",
        "Under the hood, PyTorch *performs backpropagation*, which we discussed in CSC311 and will review\n",
        "again in the coming weeks. After `loss.backward()` is computed, tensors like\n",
        "`model.bias` and `model.weights` will have gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1912c2d2",
      "metadata": {
        "id": "1912c2d2"
      },
      "outputs": [],
      "source": [
        "# recreate the model to clean up some hidden variables\n",
        "model = nn.Linear(in_features=784, out_features=10)\n",
        "\n",
        "x = train_data[0][0].reshape(1, 784)        # input\n",
        "t = torch.Tensor([train_data[0][1]]).long() # target label, represented as an integer index\n",
        "z = model(x.reshape(-1, 784))               # prediction logit\n",
        "\n",
        "loss = criterion(z, t)\n",
        "print(loss)\n",
        "\n",
        "loss.backward() # propagate the gradients with respect to the loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86efe5a2",
      "metadata": {
        "id": "86efe5a2"
      },
      "outputs": [],
      "source": [
        "print(model.bias.grad) # the gradient of the loss with respect to the bias vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3959d19",
      "metadata": {
        "id": "e3959d19"
      },
      "outputs": [],
      "source": [
        "print(model.weight.grad) # the gradient of the loss with respect to the weight matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d24d7550",
      "metadata": {
        "id": "d24d7550"
      },
      "source": [
        "**Graded Task**: Verify that `model.bias.grad` is correct by computing this gradient explicitly\n",
        "in PyTorch. You may wish to begin by reviewing your past CSC311 notes, or by using calculus to\n",
        "find an expression to represent this quantity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04c29760",
      "metadata": {
        "id": "04c29760"
      },
      "outputs": [],
      "source": [
        "t_onehot = torch.eye(10)[t] # You may find this useful. (Why? What does this quantity represent)\n",
        "model_bias_grad = None      # TODO\n",
        "\n",
        "print(model_bias_grad) # should be the same as model.bias.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a323fd4",
      "metadata": {
        "id": "6a323fd4"
      },
      "source": [
        "The gradient computation works in a vectorized setting as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3891de8",
      "metadata": {
        "id": "f3891de8"
      },
      "outputs": [],
      "source": [
        "# create a data set with 3 inputs, 3 targets\n",
        "\n",
        "x1 = train_data[0][0].reshape(1, 784)\n",
        "x2 = train_data[1][0].reshape(1, 784)\n",
        "x3 = train_data[1][0].reshape(1, 784)\n",
        "X = torch.cat([x1, x2, x3]).reshape(-1, 784)\n",
        "t = torch.Tensor([train_data[0][1], train_data[1][1], train_data[2][1]]).long()\n",
        "\n",
        "print(X.shape)\n",
        "print(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9be73b31",
      "metadata": {
        "id": "9be73b31"
      },
      "source": [
        "The average loss (mean) across the data points are shown:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75c893ae",
      "metadata": {
        "id": "75c893ae"
      },
      "outputs": [],
      "source": [
        "model = nn.Linear(in_features=784, out_features=10)\n",
        "z = model(X)\n",
        "loss = criterion(z, t)\n",
        "print(loss)\n",
        "\n",
        "loss.backward() # propagate the gradients with respect to the loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12b51dba",
      "metadata": {
        "id": "12b51dba"
      },
      "source": [
        "**Graded Task**: Verify that `model.bias.grad` is correct when using vectorized input\n",
        "by computing this gradient explicitly in PyTorch. Again, we recommend working this out\n",
        "by hand first. After you have done so, you may find the function `torch.sum()` or\n",
        "`torch.mean()` helpful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53120c97",
      "metadata": {
        "id": "53120c97"
      },
      "outputs": [],
      "source": [
        "t_onehot = torch.eye(10)[t] # TODO: understand the shape of this quantity before using it\n",
        "model_bias_grad = None      # TODO\n",
        "\n",
        "print(model_bias_grad)\n",
        "print(model.bias.grad) # should be the same as above"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaef39cc",
      "metadata": {
        "id": "eaef39cc"
      },
      "source": [
        "## Part 4. Neural Network Training via Stochastic Gradient Descent.\n",
        "\n",
        "We are almost ready to use PyTorch to train the model. One last piece that we need is an\n",
        "optimizer that updates the model parameter based on the gradient. This update can be done\n",
        "in different ways, and the most basic approach discussed in CSC311 was using **gradient descent**.\n",
        "\n",
        "$${\\bf W} \\leftarrow {\\bf W} - \\lambda \\frac{\\partial \\mathcal{L}}{\\partial {\\bf W}}$$\n",
        "\n",
        "When we use the entire training data set to compute the gradient of the mean loss (with respect\n",
        " to each parameter), we call the approach **full batch gradient descent**.\n",
        "However, this approach is expensive if we have a large training set.\n",
        "Typically, we approximate this mean loss using a **minibatch**, or a small sample of the training set.\n",
        "Using gradient descent with this approximate gradient is called **stochastic gradient descent**.\n",
        "\n",
        "PyTorch has built-in classes inside the package `torch.optim` to perform\n",
        "these gradient update steps. We will use the `SGD` class. Initializing this class\n",
        "requires a few things, including the list of model parameters that we want to optimize.\n",
        "For us, the list of parameters to optimize include `model.weight` and `model.bias`, which\n",
        "we can obtain by calling `model.parameters()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6dce9c6",
      "metadata": {
        "id": "a6dce9c6"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), # the parameters to optimize\n",
        "                      lr=0.005)           # the learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "913c955e",
      "metadata": {
        "id": "913c955e"
      },
      "source": [
        "There are two important optimizer methods that we will use.\n",
        "First is the `optimizer.zero_grad()` method, which clears the `.grad` attribute\n",
        "of the parameters. Let's see how it works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd11c2c4",
      "metadata": {
        "id": "bd11c2c4"
      },
      "outputs": [],
      "source": [
        "print(model.bias.grad) # should be a nonzero value from above\n",
        "\n",
        "optimizer.zero_grad()\n",
        "\n",
        "print(model.bias.grad) # should be cleared"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "434ce3b2",
      "metadata": {
        "id": "434ce3b2"
      },
      "source": [
        "The other method is the `step()` method, which performs the actual gradient descent update."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05055011",
      "metadata": {
        "id": "05055011"
      },
      "outputs": [],
      "source": [
        "model = nn.Linear(in_features=784, out_features=10)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.005)\n",
        "\n",
        "print(model.bias)\n",
        "\n",
        "z = model(X)\n",
        "loss = criterion(z, t)\n",
        "loss.backward()\n",
        "\n",
        "print(model.bias.grad) # gradient\n",
        "print(model.bias - 0.005 * model.bias.grad) # this should be the updated value of model.bias\n",
        "\n",
        "optimizer.step()\n",
        "\n",
        "print(model.bias) # should be different compared to above"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad951783",
      "metadata": {
        "id": "ad951783"
      },
      "source": [
        "Now that we have everything in place, we are ready to write the training loop:\n",
        "\n",
        "**Graded Task**: Complete the function below, which trains the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74dd6115",
      "metadata": {
        "id": "74dd6115"
      },
      "outputs": [],
      "source": [
        "def train_model(model,\n",
        "                train_data,\n",
        "                val_data,\n",
        "                learning_rate=0.005,\n",
        "                batch_size=100,\n",
        "                num_epochs=10,\n",
        "                plot_every=10):\n",
        "    \"\"\"\n",
        "    Train the PyTorch model `model` using the training data `train_data` and the\n",
        "    corresponding hyperparameters. Report training loss, training accuracy, and\n",
        "    validation accuracy every `plot_every` iterations.\n",
        "    \"\"\"\n",
        "    train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=True) # reshuffle minibatches every epoch\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # these lists will be used to track the training progress\n",
        "    # and to plot the training curve\n",
        "    iters, train_loss, train_acc, val_acc = [], [], [], []\n",
        "    iter_count = 0 # count the number of iterations that has passed\n",
        "\n",
        "    try:\n",
        "        for e in range(num_epochs):\n",
        "            for i, (images, labels) in enumerate(train_loader):\n",
        "                z = None # TODO\n",
        "\n",
        "                loss = None # TODO\n",
        "\n",
        "                loss.backward() # propagate the gradients\n",
        "                optimizer.step() # update the parameters\n",
        "                optimizer.zero_grad() # clean up accumualted gradients\n",
        "\n",
        "                iter_count += 1\n",
        "                if iter_count % plot_every == 0:\n",
        "                    iters.append(iter_count)\n",
        "                    train_loss.append(float(loss))\n",
        "                    train_acc.append(accuracy(model, train_data))\n",
        "                    val_acc.append(accuracy(model, val_data))\n",
        "    finally:\n",
        "        # This try/finally block is to display the training curve\n",
        "        # even if training is interrupted\n",
        "        plt.figure()\n",
        "        plt.plot(iters[:len(train_loss)], train_loss)\n",
        "        plt.title(\"Loss over iterations\")\n",
        "        plt.xlabel(\"Iterations\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "\n",
        "        plt.figure()\n",
        "        plt.plot(iters[:len(train_acc)], train_acc)\n",
        "        plt.plot(iters[:len(val_acc)], val_acc)\n",
        "        plt.title(\"Accuracy over iterations\")\n",
        "        plt.xlabel(\"Iterations\")\n",
        "        plt.ylabel(\"Accuracy\")\n",
        "        plt.legend([\"Train\", \"Validation\"])\n",
        "\n",
        "model = nn.Linear(784, 10)\n",
        "train_model(model, train_data, val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1adc6acc",
      "metadata": {
        "id": "1adc6acc"
      },
      "source": [
        "## Part 5. Hyperparameter Tuning\n",
        "\n",
        "Our training process is not yet complete.\n",
        "In general, the performance of machine learning models depend heavily on\n",
        "the hyperparameter settings used.\n",
        "Hyperparameters are settings that cannot be tuned via gradient descent in a straightforward way.\n",
        "These settings can affect the model architecture, but may also affect the optimization process.\n",
        "\n",
        "**Graded Task**: What are some examples of hyperparameters that affect the model architecture of a model?\n",
        "You may consider some hyperparameters that you learned about in CSC311. List at least 3 examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2ce4cb4",
      "metadata": {
        "id": "f2ce4cb4"
      },
      "outputs": [],
      "source": [
        "# TODO: List at least 3 examples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6becdd0",
      "metadata": {
        "id": "a6becdd0"
      },
      "source": [
        "**Task**: What are some examples of hyperparameters that affect the optimization process?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cde3a024",
      "metadata": {
        "id": "cde3a024"
      },
      "outputs": [],
      "source": [
        "# TODO: List at least 2 examples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2c5bf3e",
      "metadata": {
        "id": "a2c5bf3e"
      },
      "source": [
        "Model architecture related hyperparameters are important to tune and should *not* be neglected in\n",
        "practical application.  However, since we are working with a linear model right now, we are limited\n",
        "in this lab to exploring the optimization hyperparameters.\n",
        "\n",
        "**Task**: What happens if the learning rate is too small?\n",
        "Provide an example training curve by calling `train_model` with a low learning rate,\n",
        "and describe the features of the training curve that you see."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb15c24b",
      "metadata": {
        "id": "eb15c24b"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2db58e1e",
      "metadata": {
        "id": "2db58e1e"
      },
      "source": [
        "**Graded Task**: What happens if the learning rate is too large?\n",
        "Provide an example training curve by calling `train_model` with a large learning rate,\n",
        "and describe the features of the training curve that you see."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff112095",
      "metadata": {
        "id": "ff112095"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84d185bd",
      "metadata": {
        "id": "84d185bd"
      },
      "source": [
        "Hyperparameter choices interact with one another.\n",
        "Thus, practitioners use a strategy called **grid search** to try\n",
        "all variations of hyperparameters from a set of hyperparameters.\n",
        "We will not do that here since linear models don't yet have many hyperparameters\n",
        "to work with.\n",
        "\n",
        "**Task**: Choose the best model that you have trained. Typically we make this choice\n",
        "using the validation accuracy. To understand how well the model you choose would\n",
        "generalize to unseen data, we use the *test data*.\n",
        "Compute the test accuracy for this model by calling the function `accuracy()`\n",
        "on the model and the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab7f073b",
      "metadata": {
        "id": "ab7f073b"
      },
      "outputs": [],
      "source": [
        "# TODO: Compute the test accuracy"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}